\chapter{Methodology}
\label{chap:methodology}

This chapter will present the methodology used throughout the thesis.
It will be articulated in different parts:
the first step will involve building the model
and finding a suitable method for retrieving the parameters from data.
After this, the method must be tested on several different scenarios of increasing complexity.
These include an initial synthetic dataset built accordingly to the model,
data simulated with a generic traffic simulator and, eventually,
real data from the sources described above.

The first step is thus giving an overview of the methods that are suitable for inferring the parameters from the data,
and explaining what was the chosen method and the reasons that determined its choice.

\section{Inference Method}
\label{sec:inference}

For retrieving parameters from the data, different methods could be used,
each one of which is better suited for a different problem.
In order to choose one of these methods, a more formal statement of the problem is necessary.

In the next lines, the inference problem is more formally stated,
in order to acquire a deeper understanding of it and to be able to choose the right inference method.
After that, the available methods will be presented and the most suitable method will be chosen from them.

\subsection{Theoretical Preliminaries}
\label{sec:theo_pre}

As stated in the introduction, we are assuming that each user is minimizing a cost function,
that has the form in \eqref{eq:cost_intro}:
\begin{equation}
  \label{eq:cost_init_inf}
  C(t) = \alpha(\text{travel time}) + \beta (\text{time early}) + \gamma (\text{time late})
\end{equation}
Let then \(t^*\) be the desired arrival time,
\(tt(t)\) the travel time if leaving from the origin at time \(t\) and
\([\bullet]^+\) the function that is known in machine learning as \textit{ReLU}, that is,
\([x]^+ = \max(0, x)\).

The \textit{time early} becomes thus the difference between the desired arrival time \(t^*\) and the actual arrival time \(t + tt(t)\), cut at zero:
\[\text{time early} = [t^* - tt(t) - t]^+\]
Similarly, for \textit{time late}
\[\text{time late} = [tt(t) + t - t^*]^+\]
The expression for the cost in \eqref{eq:cost_init_inf} becomes thus

\begin{equation}
  \label{eq:cost_inf}
  C(t; \alpha, \beta, \gamma, t^*) = \alpha tt(t) + \beta[t^* - tt(t) - t]^+ + \gamma[tt(t) + t - t^*]^+
\end{equation}

Each user is thus minimizing (over the variable \(t\))
a function that is parametric, with four different parameters:
the user preferences \(\alpha, \beta, \gamma\) and the desired arrival time \(t^*\).
This minimization will yield an optimal departure time \(t^{opt}\),
that will vary in function of the parameters:
\begin{equation}
  \label{eq:t_opt}
  t^{opt} = t^{opt}(\alpha, \beta, \gamma, t^*) = \min_{t \in (0, 24)} C(t; \alpha, \beta, \gamma, t^*)
\end{equation}

Suppose now that all four parameters are distributed across the different commuters,
and the parameter of each commuter are sampled from four different random variables,
whose probability distribution functions \todo{maybe by introducing the parameter \(\theta\), the approach is already not bayesian (actually, maybe not)} are parametrized by a parameter \(\theta \in \R^n, n \geq 1\).
The minimization process yields another random variable, parametrized by the same parameter \(\theta\),
which describes the optimal departure time \(t^{opt}\).
Let \(T^{opt}\) be this random variable, and
\[ f_{T^{opt}}(t; \theta) \]
its Probability Density Function.

The inference problem is thus the following:
a dataset is given,
which is close to what would result from sampling from the random variable \(T^{opt}\).
What is the value of the parameter \(\theta\) that the best explains the dataset?

For answering this question, the parameter has to be inferred from the data,
and an approach for doing this has to be chosen.
It is well known how the inference methods are divided in Bayesian and Frequentist inference
(for a really in-depth review of the discussion, see \cite{bandyopadhyay2011philosophy}).
The main inference methods are thus either Bayesian methods (for an overview, see \cite{gelman2013bayesian}) or simpler Maximum Likelihood \parencite{doi:10.1098/rsta.1922.0009}.
The following sections describe better the different approaches,
and discuss how are they applicable to the examined problem

\subsection{Maximum Likelihood Estimation}
\label{sec:max_lik}


\subsection{Bayesian Estimation}
\label{sec:bayes}




 is worth nothing that, in our case, the approaches do not give substantially different models:
the limited information we have about the parameters would indeed not allow us to define a sufficiently informative prior,
and (as is well known and simple to verify) bayesian estimation with uninformative priors reduce to maximum likelihood estimations.
A decision was thus taken to estimate the parameters by simply maximising the likelihood,
not taking in consideration any information on the prior.

To maximise the likelihood, two main approaches are viable:
one could either analytically find the expression for the likelihood
empirical likelihood \parencite{10.1093/biomet/75.2.237}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
