\chapter{Methodology}
\label{chap:methodology}

This chapter will present the methodology used throughout the thesis.
It will be articulated in different parts:
the first step will involve building the model
and finding a suitable method for retrieving the parameters from data.
After this, the method must be tested on several different scenarios of increasing complexity.
These include an initial synthetic dataset built accordingly to the model,
data simulated with a generic traffic simulator and, eventually,
real data from the sources described above.

The first step is thus giving an overview of the methods that are suitable for inferring the parameters from the data,
and explaining what was the chosen method and the reasons that determined its choice.

\section{Inference Methods}
\label{sec:inference}

For retrieving parameters from the data, different methods could be used,
each one of which is better suited for a different problem.
In order to choose one of these methods, a more formal statement of the problem is necessary.

In the next lines, the inference problem is more formally stated,
in order to acquire a deeper understanding of it and to be able to choose the right inference method.
After that, the available methods will be presented and the most suitable method will be chosen from them.

\subsection{Theoretical Preliminaries}
\label{sec:theo_pre}

As stated in the introduction, we are assuming that each user is minimizing a cost function,
that has the form in \eqref{eq:cost_intro}:
\begin{equation}
  \label{eq:cost_init_inf}
  C(t) = \alpha(\text{travel time}) + \beta (\text{time early}) + \gamma (\text{time late})
\end{equation}
Let then \(t^*\) be the desired arrival time,
\(tt(t)\) the travel time if leaving from the origin at time \(t\) and
\([\bullet]^+\) the function that is known in machine learning as \textit{ReLU}, that is,
\([x]^+ = \max(0, x)\).

The \textit{time early} becomes thus the difference between the desired arrival time \(t^*\) and the actual arrival time \(t + tt(t)\), cut at zero:
\[\text{time early} = [t^* - tt(t) - t]^+\]
Similarly, for \textit{time late}
\[\text{time late} = [tt(t) + t - t^*]^+\]
The expression for the cost in \eqref{eq:cost_init_inf} becomes thus

\begin{equation}
  \label{eq:cost_inf}
  C(t; \alpha, \beta, \gamma, t^*) = \alpha tt(t) + \beta[t^* - tt(t) - t]^+ + \gamma[tt(t) + t - t^*]^+
\end{equation}

Each user is thus minimizing (over the variable \(t\))
a function that is parametric, with four different parameters:
the user preferences \(\alpha, \beta, \gamma\) and the desired arrival time \(t^*\).
This minimization will yield an optimal departure time \(t^{opt}\),
that will vary in function of the parameters:
\begin{equation}
  \label{eq:t_opt}
  t^{opt} = t^{opt}(\alpha, \beta, \gamma, t^*) = \min_{t \in (0, 24)} C(t; \alpha, \beta, \gamma, t^*)
\end{equation}

Suppose now that all four parameters are distributed across the different commuters,
and the parameter of each commuter are sampled from four different random variables,
whose probability distribution functions \todo{maybe by introducing the parameter \(\theta\), the approach is already not bayesian (actually, maybe not)} are parametrized by a parameter \(\theta \in \R^n, n \geq 1\).
The minimization process yields another random variable, parametrized by the same parameter \(\theta\),
which describes the optimal departure time \(t^{opt}\).
Let \(T^{opt}\) be this random variable, and
\begin{equation}
  \label{eq:pdf_opt}
  f_{T^{opt}}(t; \theta)
\end{equation}
its Probability Density Function.

The inference problem is thus the following:
a dataset is given,
which is close to what would result from sampling from the random variable \(T^{opt}\).
What is the value of the parameter \(\theta\) that the best explains the dataset?

For answering this question, the parameter has to be inferred from the data,
and an approach for doing this has to be chosen.
It is well known how the inference methods are divided in Bayesian and Frequentist inference
(for a really in-depth review of the discussion, see \cite{bandyopadhyay2011philosophy}).
The main inference methods are thus either Bayesian methods (for an overview, see \cite{gelman2013bayesian}) or simpler Maximum Likelihood \parencite{doi:10.1098/rsta.1922.0009}.
The following sections describe better the different approaches,
and discuss how are they applicable to the examined problem

\subsection{Maximum Likelihood Estimation}
\label{sec:max_lik}
The Maximum Likelihood Estimation is the simplest, older and probably most widely used method examined here.

The methods relies on a simple idea:
for each value of the parameter \(\theta\),
each data point will have a certain \textit{likelihood} (or, in the discrete case, probability) of being a realization of the random variable \(T^{opt}\).
It is thus a good idea to look for the value of \(\theta\) that maximises the likelihood for the whole given dataset:
this value will thus be called the Maximum Likelihood Estimator (MLE) estimate.

More formally, let \(f_{T^{opt}}(t; \theta)\) be (as in \eqref{eq:pdf_opt}) the probability density function for the random variable \(T^{opt}\).

The \textit{likelihood} of the parameter \(\theta\) given the observation of a data point \(s\) is defined as
\begin{equation}
  \label{eq:lik_one_point}
  \lik(\theta | s)  = f_{T^{opt}}(s; \theta)
\end{equation}

More generally, let
\[S = \{t_1, \dots, t_N\}\]
be the given dataset of the actual departure times,
that can be thought of as independent samples of the random variable \(T^{opt}\).

\todo{Terribly written}
Since the joint probability of independent events can be computed by multiplying the probabilities of the individual events,
the likelihood of the parameter \(\theta\) (given the dataset \(S\)) will be defined as
\begin{equation}
  \label{eq:lik_def}
  \lik(\theta | S) = \prod_{s \in S} f_{T^{opt}}(s; \theta)
\end{equation}

The MLE estimate \(\hat{\theta}\) is now simply defined as the value of the parameter \(\theta\) that maximises the likelihood:
\begin{equation}
  \label{eq:mle_estimate}
  \hat{\theta} = \argmax_\theta \lik(\theta | S)
\end{equation}

The maximization of the likelihood function can be performed in different ways:
when possible, an analytical maximization could be the fastest and most precise estimation method.
Finding an analytical solution is anyway not always viable,
since the function can have a convoluted analytical expression.
Widely used methods include thus numerical optimization methods,
often via gradient descent
(in the cases in which the likelihood is differentiable, and the gradient is easy to compute)
or even via gradient-free optimizers using, for instance,
the simplex method \parencite{10.1093/comjnl/7.4.308}.

Even being this the simplest method, it presents some challenges:
the likelihood is indeed not trivial to estimate,
as the random variable \(T^{opt}\) is defined itself as the minimization of a cost function.
Moreover, the computation of the likelihood has to be done fast enough to allow an optimizer to run on it,
in case the computed likelihood does not allow an analytical maximization (that is highly likely,
given how the likelihood is defined).

These problems, which will equally affect the other estimation methods,
will be tackled in section \todo{Put section number}LIKELIHOOD.
I will thus here present the other main estimation method.

\subsection{Bayesian Estimation}
\label{sec:bayes}

The bayesian approach is a philosophically (beyond practical) different way of looking at the problem.
According to this approach, parameters are not indeed seen as real variables that can maximised,
but rather as random variables themselves,
some prior knowledge of which can be integrated in the model.

Consider, for instance, an inference problem in which
the height \(h\) of a group of people has to be estimated.
A likelihood that is maximised for an unrealistic value of the parameter (e. g., \(h = 15 \unit{\cm}\)) is extremely unhelpful when doing an MLE,
and would lead to a wrong estimation.
Using bayesian methods, on the other hand,
it is possible to consider some prior knowledge about the parameter before doing the estimation
(for instance, the parameter can be believed to be distributed close to the average height of a person).
Data will thus enhance this knowledge,
specified trough the choice of a \textit{prior} distribution,
and transform it to another distribution, called \textit{posterior} distribution.

Sadly, in our case, having a meaningful prior distribution is quite difficult:
as noted in section \ref{cha:introduction},
knowledge about the distribution of them is limited.
A decision was thus taken to estimate the parameters by simply maximising the likelihood,
not taking in consideration any information on the prior.

It is thus important to study the likelihood,
in order to be able to perform the parameter estimation at the core of the project.

\section{Likelihood Estimation}
\label{sec:lik_est}


To maximise the likelihood, two main approaches are viable:
one could either analytically find the expression for the likelihood
empirical likelihood \parencite{10.1093/biomet/75.2.237}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
