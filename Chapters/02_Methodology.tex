\chapter{Methodology}
\label{chap:methodology}

This chapter will present the methodology used throughout the thesis.
It will be articulated in different parts:
the first step will involve building the model
and finding a suitable method for retrieving the parameters from data.
After this, the method must be tested on several different scenarios of increasing complexity.
These include an initial synthetic dataset built accordingly to the model,
data simulated with a generic traffic simulator and, eventually,
real data from the sources described above.

The first step is thus giving an overview of the methods that are suitable for inferring the parameters from the data,
and explaining what was the chosen method and the reasons that determined its choice.

\section{Inference Methods}
\label{sec:inference}

For retrieving parameters from the data, different methods could be used,
each one of which is better suited for a different problem.
In order to choose one of these methods, a more formal statement of the problem is necessary.

In the next lines, the inference problem is more formally stated,
in order to acquire a deeper understanding of it and to be able to choose the right inference method.
After that, the available methods will be presented and the most suitable method will be chosen from them.

\subsection{Theoretical Preliminaries}
\label{sec:theo_pre}

As stated in the introduction, we are assuming that each user is minimizing a cost function,
that has the form in \eqref{eq:cost_intro}:
\begin{equation}
  \label{eq:cost_init_inf}
  C(t) = \alpha(\text{travel time}) + \beta (\text{time early}) + \gamma (\text{time late})
\end{equation}
Let then \(t^*\) be the desired arrival time,
\(tt(t)\) the travel time if leaving from the origin at time \(t\) and
\([\bullet]^+\) the function that is known in machine learning as \textit{ReLU}, that is,
\([x]^+ = \max(0, x)\).

The \textit{time early} becomes thus the difference between the desired arrival time \(t^*\) and the actual arrival time \(t + tt(t)\), cut at zero:
\[\text{time early} = [t^* - tt(t) - t]^+\]
Similarly, for \textit{time late}
\[\text{time late} = [tt(t) + t - t^*]^+\]
The expression for the cost in \eqref{eq:cost_init_inf} becomes thus

\begin{equation}
  \label{eq:cost_inf}
  C(t; \alpha, \beta, \gamma, t^*) = \alpha tt(t) + \beta[t^* - tt(t) - t]^+ + \gamma[tt(t) + t - t^*]^+
\end{equation}

Each user is thus minimizing (over the variable \(t\))
a function that is parametric, with four different parameters:
the user preferences \(\alpha, \beta, \gamma\) and the desired arrival time \(t^*\).
This minimization will yield an optimal departure time \(t^{opt}\),
that will vary in function of the parameters:
\begin{equation}
  \label{eq:t_opt}
  t^{opt} = t^{opt}(\alpha, \beta, \gamma, t^*) = \min_{t \in (0, 24)} C(t; \alpha, \beta, \gamma, t^*)
\end{equation}

Suppose now that all four parameters are distributed across the different commuters,
and the parameter of each commuter are sampled from four different random variables,
whose probability distribution functions \todo{maybe by introducing the parameter \(\theta\), the approach is already not bayesian (actually, maybe not)} are parametrized by a parameter \(\theta \in \R^n, n \geq 1\).
The minimization process yields another random variable, parametrized by the same parameter \(\theta\),
which describes the optimal departure time \(t^{opt}\).
Let \(T^{opt}\) be this random variable, and
\begin{equation}
  \label{eq:pdf_opt}
  f_{T^{opt}}(t; \theta)
\end{equation}
its Probability Density Function.

The inference problem is thus the following:
a dataset is given,
which is close to what would result from sampling from the random variable \(T^{opt}\).
What is the value of the parameter \(\theta\) that the best explains the dataset?

For answering this question, the parameter has to be inferred from the data,
and an approach for doing this has to be chosen.
It is well known how the inference methods are divided in Bayesian and Frequentist inference
(for a really in-depth review of the discussion, see \cite{bandyopadhyay2011philosophy}).
The main inference methods are thus either Bayesian methods (for an overview, see \cite{gelman2013bayesian}) or simpler Maximum Likelihood \parencite{doi:10.1098/rsta.1922.0009}.
The following sections describe better the different approaches,
and discuss how are they applicable to the examined problem

\subsection{Maximum Likelihood Estimation}
\label{sec:max_lik}
The Maximum Likelihood Estimation is the simplest, older and probably most widely used method examined here.

The methods relies on a simple idea:
for each value of the parameter \(\theta\),
each data point will have a certain \textit{likelihood} (or, in the discrete case, probability) of being a realization of the random variable \(T^{opt}\).
It is thus a good idea to look for the value of \(\theta\) that maximises the likelihood for the whole given dataset:
this value will thus be called the Maximum Likelihood Estimator (MLE) estimate.

More formally, let \(f_{T^{opt}}(t; \theta)\) be (as in \eqref{eq:pdf_opt}) the probability density function for the random variable \(T^{opt}\).

The \textit{likelihood} of the parameter \(\theta\) given the observation of a data point \(s\) is defined as
\begin{equation}
  \label{eq:lik_one_point}
  \lik(\theta | s)  = f_{T^{opt}}(s; \theta)
\end{equation}

More generally, let
\[S = \{t_1, \dots, t_N\}\]
be the given dataset of the actual departure times,
that can be thought of as independent samples of the random variable \(T^{opt}\).

\todo{Terribly written}
Since the joint probability of independent events can be computed by multiplying the probabilities of the individual events,
the likelihood of the parameter \(\theta\) (given the dataset \(S\)) will be defined as
\begin{equation}
  \label{eq:lik_def}
  \lik(\theta | S) = \prod_{s \in S} f_{T^{opt}}(s; \theta)
\end{equation}

The MLE estimate \(\hat{\theta}\) is now simply defined as the value of the parameter \(\theta\) that maximises the likelihood:
\begin{equation}
  \label{eq:mle_estimate}
  \hat{\theta} = \argmax_\theta \lik(\theta | S)
\end{equation}

The maximization of the likelihood function can be performed in different ways:
when possible, an analytical maximization could be the fastest and most precise estimation method.
Finding an analytical solution is anyway not always viable,
since the function can have a convoluted analytical expression.
Widely used methods include thus numerical optimization methods,
often via gradient descent
(in the cases in which the likelihood is differentiable, and the gradient is easy to compute)
or even via gradient-free optimizers using, for instance,
the simplex method \parencite{10.1093/comjnl/7.4.308}.
The optimization methods will be investigated further in section \ref{sec:max_lik_meth}

Even being this the simplest method, it presents some challenges:
the likelihood is indeed not trivial to estimate,
as the random variable \(T^{opt}\) is defined itself as the minimization of a cost function.
Moreover, the computation of the likelihood has to be done fast enough to allow an optimizer to run on it,
in case the computed likelihood does not allow an analytical maximization (that is highly likely,
given how the likelihood is defined).

These problems, which will equally affect the other estimation methods,
will be tackled in section \todo{Put section number}LIKELIHOOD.
I will thus here present the other main estimation method.

\subsection{Bayesian Estimation}
\label{sec:bayes}

The bayesian approach is a philosophically (beyond practical) different way of looking at the problem.
According to this approach, parameters are not indeed seen as real variables that can maximised,
but rather as random variables themselves,
some prior knowledge of which can be integrated in the model.

Consider, for instance, an inference problem in which
the height \(h\) of a group of people has to be estimated.
A likelihood that is maximised for an unrealistic value of the parameter (e. g., \(h = 15 \unit{\cm}\)) is extremely unhelpful when doing an MLE,
and would lead to a wrong estimation.
Using bayesian methods, on the other hand,
it is possible to consider some prior knowledge about the parameter before doing the estimation
(for instance, the parameter can be believed to be distributed close to the average height of a person).
Data will thus enhance this knowledge,
specified trough the choice of a \textit{prior} distribution,
and transform it to another distribution, called \textit{posterior} distribution.

Sadly, in our case, having a meaningful prior distribution is quite difficult:
as noted in section \ref{cha:introduction},
knowledge about the distribution of them is limited.
A decision was thus taken to estimate the parameters by simply maximising the likelihood,
not taking in consideration any information on the prior.

It is thus important to study the likelihood,
in order to be able to perform the parameter estimation at the core of the project.

\section{ Estimation of the Likelihood}
\label{sec:lik_est}
To estimate the likelihood, two main approaches are viable:
one could either analytically find the expression for the likelihood,
that requires doing a more extensive and heavier theoretical work,
but yields a more precise and (potentially) better performing method,
or choose to sample data points (by directly minimizing the cost function)
to perform a subsequent maximization on what is known as \textit{empirical likelihood} \parencite{10.1093/biomet/75.2.237}:
a nonparametric estimation of the likelihood function from a set of samples.

While the standard approach could provide a way of quickly computing the likelihood,
(enhancing as well the knowledge of the subject),
empirically approximating it could yield an acceptable approximation and would not require as much theoretical understanding as the direct computation.
But, since theoretical understanding of the problem is not something to be neglected,
both methods deserve to be presented.

\subsection{Empirical Approximation of the Likelihood}
\label{sec:emp_approx_lik}

Empirical estimation of the likelihood is a method that was firstly defined by \textcite{10.1093/biomet/75.2.237}.
It consists in the estimation of the likelihood based on the \textit{empirical Cumulative Distribution Function} (eCDF)  \(\hat{F}(x)\) (simply, the ratio between the number of samples before the point in which the function is evaluated and the number of the total samples),
and only requires an arbitrary number of independent and identically distributed (i.i.d.) samples.
\missingfigure{Figure about eCDF vs CDF?}
The next paragraph will briefly explain how does the method work.

Consider a set \(S = \{t_1, \dots, t_N\}\) of \(N\) i.i.d. samples from the distribution we want to calculate the likelihood of.
The eCDF for the set of samples \(S\) will be defined as
\begin{equation}
  \label{eq:emp_cdf}
  \hat{F}(t) = \frac{1}{N}\sum_{i = 1}^N \mathbb{1}_{t_i \leq t}
\end{equation}
where \(\mathbb{1}_A\) is the \textit{indicator} of event \(A\),
evaluating to 1 if \(A\) occurs, and to 0 otherwise.

The empirical likelihood of a data point x can now simply be defined as

\begin{equation}
  \label{eq:emp_lik}
  L(x) = \frac{\hat{F}(x) - \hat{F}(x - \delta)}{\delta}
\end{equation}
where \(\delta\) is a parameter,
to be set as small as the size of the set \(S\) allows
(note indeed that, if no samples appear between \(x\) and \(x - \delta\), the likelihood \(L(x)\) will be equal to zero).
For a more in-depth discussion of the empirical likelihood and its properties, see \textcite{annurev:/content/journals/10.1146/annurev-statistics-040720-024710}.

This method is certainly relevant for the problem examined in this thesis:
the likelihood could indeed be computed regardless of the distributions of the parameters \(\alpha, \beta, \gamma, t^*\),
as long as sampling from their distribution is easy enough.
Once a sample has been taken from each of their distribution,
minimizing the cost function yield a data point in the set \(S\).
By drawing independent samples from the chosen distributions for the parameters,
the set \(S\) can be built of a sufficient size.
The likelihood of the given dataset can then be estimated against the sampled data,
and subsequently maximized.

The biggest drawback of this effort concerns the computational complexity:
for each estimation of the likelihood function,
a sufficient number of samples has to be drawn from the hypothesized parameter distributions,
and a minimization has to be performed over the cost function after each set of parameters is drawn.
Moreover, this method would not allow the computation of the gradient of the likelihood;
the minimization would thus have to be computed using gradient-free optimizers,
which typically involves a big number of function evaluations \parencite{Larson_Menickelly_Wild_2019} which would be,
in this case, particularly complex.

This problem could be avoided by using modern high performance computing frameworks,
and powerful machines.
It is anyway a major drawback and suggests, if possible, the adoption of alternative methods.

\subsection{Direct Computation of the Likelihood}
\label{sec:lik_comp}

As mentioned earlier, the obvious method of computing a likelihood is the direct computation of it.

Even looking obvious, this approach is not completely trivial to develop:
note indeed that the random variable \(T^{opt}\) is defined by minimizing the cost function,
in which several other random variables appear.

For obtaining an analytical expression for the Probability Density Function \(f_{T^{opt}}(t; \theta)\), different steps are thus required:
first of all, a complete understanding of how changing the different parameters affects the cost function has to be developed.
Subsequently, where the cost is minimized has to be studied;
the dependency of the point that minimizes the cost on each one of the parameters has to be characterized.
Lastly, the characterization has to be extended to the case in which the parameters are distributed rather than real values.

This process could be particularly laborious,
but once done it could provide an elegant and precise estimation method:
having a full characterization of the points minimizing the cost function \(C(t; \alpha, \beta, \gamma, t^*)\) would indeed require a deep knowledge of the problem,
and this is, herein, interesting by itself on top of being useful.

Despite being challenging,
this approach is the chosen one:
it will yield a complete characterization of the distribution of the random variable \(T^{opt}\).

Once the analytical expression for the likelihood is found,
the estimation is not done at all:
computing the likelihood will indeed require computations that can only be numerically approximated,
and whose approximation with a good precision requires a big computational effort.
The following section will delve a bit deeper in the problem,
and presents some solutions that can help in mitigating the issues.

\subsection{Speeding Up the Likelihood Computation}
\label{sec:lik_speed}
\todo{I don't understand if this section has to go in methods or results: I am assuming the likelihood to be known, but it is not...}
As stated in the previous paragraph, numerically computing the Probability Density Function of the variable \(T^{opt}\) will,
by itself, not be a trivial task.
Moreover, the computation has to be repeated a big number of times for performing the final MLE estimate for the parameter \(\theta\):
the value of the pdf has indeed to be computed for each point in the dataset,
and once this computation has been done the final result has to be maximised,
requiring the whole computation to be performed as many times as the maximization algorithm requires to.
Moreover, being able to perform a gradient descent on the result of the likelihood would greatly reduce the number of times the computation is performed,
while the simpler algorithms would be considerably slower by requiring a bigger number of function evaluations.

The problems in the evaluation of the likelihood are mainly two:
the first one is the need of evaluating integrals of function whose primitive is not analytically computable;
the second one is the need of inverting some functions whose inverse does not have a closed form analytical expression.

Both problems are solvable,
since there exists methods that find the solutions with arbitrary precision,
as long as the functions are behaved well enough (that is the case in this problem).

Regarding the integration, the solution can be approximated by using the \todo{add figure for this?} trapezoid method,
that requires a finite number of function evaluations and yields a good estimate of the integral
(for a deeper discussion on this, see \cite{Sueli_Mayers_2003}).
An approximated integral via the trapezoid method can be thus computed and,
if gradient descent has to be performed on its result,
the derivative of it can be computed.
The differentiation is anyway not feasible analytically, since it involves a great number of terms.
In order to properly differentiate it,
an automatic differentiation framework is needed.

For what concerns inverting functions, the problem is again computationally tractable,
as the inverse can be approximated iteratively by using a bisection method or,
in some particular cases, a gradient descent algorithm.
Anyway, differentiating the result of an iterative method does not even have a general meaning,
and is thus not doable.
A method recently developed at Google,
called implicit differentiation \parencite{DBLP:journals/corr/abs-2105-15183},
allows anyway to numerically approximate the gradient of the result of optimization techniques.
The method is efficiently implemented for the automatic differentiation framework JAX \parencite{jax2018github}
that, on top of allowing the computation of gradients,
speeds up by several order of magnitude the written code (compared to standard python implementations)
by leveraging just-in-time compilation and hardware acceleration.
Utilizing this library will be fundamental in the development of the thesis,
by allowing the computations to be ran in tolerable times even in normal, consumer-grade machines.

After the likelihood has been efficiently computed,
inferring the parameters requires it to be maximised.
An overview of the possible maximisation methods is thus given in the following section.

\section{Maximization of the Likelihood}
\label{sec:max_lik_meth}

The ideal way of finding the maximum of the likelihood would be differentiating it,
invert the derivative and analytically compute the result.
This is sadly not possible in general,
since the expression for the likelihood could be very convoluted and,
as a consequence,
the derivative can become analytically intractable.

To solve this problem,
the maximum will be numerically approximated.
Different techniques can be used for the approximation of it,
and they will be presented in this section.

\subsection{Grid Search}
\label{sec:grid_search}

The most basic and intuitive solution for maximising a function (after randomly selecting a point to be the maximum) is grid search:
the function is evaluated on a set of equally spaced points spread over its domain,
and the point in which the function assumes the highest value is chosen as the approximated maximum.

Despite this being a naive solution,
this method offers some advantages compared to other more sophisticated algorithms.
A grid search is indeed unlikely to output a local minimum,
since no information about wether the function is growing is ever used.
Moreover, the algorithm is not inherently iterative,
and as such greatly parallelizable.

On the other hand,
this solution has an undeniable drawback:
the precision of the convergence is really low,
and improving it requires an enormous computational effort.
Some techniques have been proposed to overcome this limitation
(as in \cite{pathak2024randomizedgridsearchhyperparametertuning,poštuvan2022adagridadaptivegridsearch}),
but often combining it with an iterative maximization algorithm offers the best results.

Iterative methods are thus examined in the next sections.

\subsection{Gradient Descent Methods}
\label{sec:grad_desc}

Gradient descent (with the various modified versions of it) is arguably the most widely known and used method in optimization \parencite{ruder2017overviewgradientdescentoptimization}.
It is based on a simple idea:
it is easy to prove that, for a differentiable function,
the direction in which the value of the function increases the fastest (clearly, locally)
is the one pointed by the gradient.
Once the gradient has been computed, evaluating it at a point gives thus a suggestion (as long as the function is behaved well enough) on the direction in which the maximum can be computed.

More formally, let
\begin{align*}
  J: \R^d & \rightarrow \R \\
  \theta & \mapsto J(\theta)
\end{align*}
the function that has to be minimised
(if the function has to be maximised, it is sufficient to change the sign of it).

Let \(\nabla J(\theta)\) be the gradient of it with respect to the variable \(\theta\),
and \(\theta_0 \in \R^d\) some initial value from where the optimization has to be started.
According to the gradient descent algorithm,
the current estimation is updated as follows:
\begin{equation}
  \label{eq:grad_desc}
  \theta_{i+1} = \theta_i - \alpha_i\nabla J(\theta_i)
\end{equation}
where \(\alpha_i\), called \textit{step size}, depends on the chosen optimization strategy.

The step size can be either constant, yielding a more predictable but often slower or less precise convergence,
or variable.
In the case in which the step size is not constant,
it can be changed in a variety of different ways,
defining several different gradient descent algorithms with their strength and weaknesses
\parencite{doi:10.1137/1011036,MR701288}.

For a deeper discussion on these methods, including how the step size can be chosen,
see \cite{10.5555/3317111}.

Many of the strength and weaknesses of these method are anyway common across all of them,
and the major weakness is, in many cases,
the need of computing the gradient.

In our case, the gradient of the likelihood can be computed via, as presented in section \ref{sec:lik_speed},
modern computational frameworks which allow implicit differentiation.
The methods may anyway have issues with numerical stability,
and need careful tuning for avoiding regions that are out of the domain\footnote{As reported in a conversation with a JAX developer}.
This problem could, in principle,
make the computation of the gradient not feasible.
In this case, iterative algorithms such as gradient-less optimizers could still yield good results.

\subsection{Simplex Method}
\label{sec:no_grad_opt}

It may happen, in general, that no information is available other than the function values,
making it impossible to compute the gradient,
or particularly expensive to estimate it.

For addressing this type of problems, some optimization techniques which only rely on the values of the function at some points,
without needing anything else,
have been developed.
I will focus here on the method that is arguably the most popular among them,
developed by \textcite{10.1093/comjnl/7.4.308}.
For a deeper review of the different methods, see \cite{10.5555/1508119}.

The simplex method, as its name suggests, is based on the estimation of the function on the edges of a simplex.
The simplex is simply the extension of a triangle to an arbitrary-dimensional euclidean space:
in dimension \(d\), the method will thus maintain \(d+1\) points,
and update their position at each iterate.

The update can be done in four different ways:
depending on some conditions on the value of the function at the edges,
\textit{reflection} reflect one of the points with respect to the centroid of the remaining ones;
\textit{expansion} on top of reflecting a point, moves it away from the centroid of the other edges, by a linear factor;
\textit{contraction} is similar to the expansion, but instead of moving the edge away from the centroid it moves it closer;
finally, if none of the conditions for the other updates is satisfied,
\textit{shrinking} is performed:
each point is pulled towards the best one proportionally on their distance from it.
When the simplex is lying on a \textit{flat} surface (that is, when the values of the function at its edges are close enough),
the method is terminated and the edge in which the function assumes the lowest value is returned as the minimum.

This method achieves convergence with a relatively little number of function evaluations,
since the values of the function at the edges can be stored in memory.
Despite this, it presents two major drawbacks.
The first one is about the number of function evaluations:
while the method does not require a big number of them if compared with gradient-free methods,
the number of function evaluations it requires can exceed the ones required by gradient methods by different order of magnitude.
The second drawback regards the optimum found by the method:
as found by \textcite{doi:10.1137/S1052623496303482},
the method can indeed converge to nonstationary points,
where the gradient is not null and a gradient based optimizer would escape.
This phenomenon can be reduced by tweaking the parameters,
but occurs in different situations and can be an obstacle to the proper convergence of the estimate.

This method will thus be used only when unavoidable,
and avoided in favour of gradient-based methods when possible.

Once the likelihood has been maximised,
the method has to be tested on scenarios in which the convergence can be evaluated,
in order to prove it to actually be meaningful.
The following sections will present how the evaluation will be performed,
on various scenarios of increasing complexity.

\section{Evaluation on Synthetic Data}
\label{sec:eval_synth}

The first, simplest scenario for testing the method consists in building a dataset of artificial observations of arrival time,
from a purely theoretical (and well behaved) travel time function.
Building the dataset without any connection with real scenarios won't yield important data regarding the usefulness of the method when real data will be used.
It anyway provides a nice instrument for evaluating the convergence properties of the method.
Moreover, evaluating the method on a tailored dataset efficiently separates the method from the underlying theory:
this step will enable to evaluate the method without it being affected by potential faults of the bottleneck modelling we assume.

To perform this evaluation, the first thing that needs to be done is building the dataset.

\subsection{Building the Synthetic Dataset}
\label{sec:synth_dataset}

The dataset has to be build independently from the theory developed when estimating the likelihood,
since a theory cannot be used for evaluating itself.

The most independent way to build a synthetic dataset is thus to computationally minimise the cost function for each agent,
and to find in this way the chosen departure time.

Remind that, from \eqref{eq:cost_inf}, the cost is equal to
\begin{equation*}
  C(t; \alpha, \beta, \gamma, t^*) = \alpha tt(t) + \beta[t^* - tt(t) - t]^+ + \gamma[tt(t) + t - t^*]^+
\end{equation*}

To minimize it, a specification of the travel time function \(tt(t)\) is thus needed,
on top of the parameters \(\alpha, \beta, \gamma, t^*\).
The travel time function can be chosen to have an arbitrary analytical form,
as long as it satisfies the theoretical properties one expects from it (that will be defined further ahead).
On the other hand, the parameters will be sampled from the chosen distribution,
parametrized by the parameter \(\theta\).

Once the cost function has been defined,
the minimization has to occur.
Since the cost can have up to three local minima (as shown in section \todo{Section number} THEORY1),
three different optimizers are run on it,
each one of which aimed to one of the minima.
The retrieved values are then compared, and the global minimum is found in this way.
This procedure can be run for an arbitrary number of time,
in order to obtain dataset of the wanted size \(N\).

With this method, both the size of the dataset \(N\) and the parameter \(\theta\) can be tweaked as much as needed:
convergence can in this way be evaluated for many shapes of the distribution,
to ensure the robustness of the developed method.
%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
