\chapter{Results}
\label{chap:res}

In this chapter, the results of the research will be presented.

The first step for building the model is some analytical work:
in section~\ref{sec:minimum},the points in which the cost function is minimized are studied,
and a characterization of the minima is done.
This characterization is then be used in section LIKELIHOOD\todo{Section label},
where the statistical framework is developed and the likelihood function introduced.

Once an expression for the likelihood function is found,
it can be numerically implemented:
details of the implementation, and its results on the various test types,
are shown in the following sections.

\section{Minimizing the Cost Function}
\label{sec:minimum}

For characterizing where the cost function is minimized,
it is a good idea to simplify it as much as possible.
Some theoretical preliminaries are hence necessary.

\subsection{Theoretical Preliminaries}
\label{sec:pre_minimizing}

Recall that, from equation \eqref{eq:cost_inf}, we are assuming the cost to be

\begin{equation*}
  C(t; \alpha, \beta, \gamma, t^*) = \alpha tt(t) + \beta [t^* - tt(t) - t]^+ + \gamma[tt(t) + t - t^*]^+
\end{equation*}

where

\begin{itemize}
\item \(t^*\) is the desired arrival time
\item \(\alpha\) is the value of time spent travelling
\item \(\beta\) is the value of time spent waiting at the destination
\item \(\gamma\) is the value of time arriving late at the destination
\item \(tt(t_d)\) is the time spent travelling if departing at time \(t_d\)
\item \([x]^+ = \max(0, x)\)
\end{itemize}

The first observation is that minimizing a function is equivalent to minimizing a scaled version of it:
instead of minimizing the function \(C(t_d)\),
I will study the minimization problem on the function \(C(t_d)/\alpha\) or,
equivalently, normalize the parameter \(\alpha\) to 1.

From now on a slightly different cost function will be considered:
\begin{equation}
  \label{eq:cost_no_alpha}
  C(t; \beta, \gamma, t^*) = tt(t) + \beta [t^* - tt(t) - t]^+ + \gamma[tt(t) + t - t^*]^+
\end{equation}
where the numerical values of the parameters \(\beta, \gamma\) are actually representing the values for \(\beta/\alpha, \gamma/\alpha\).

Note now that most of the cost function depends on when the user \textit{decides to arrive} rather than when the user decides to leave.
Expressing it in terms of the arrival time may thus be more natural,
and simplify the expression of the cost function.

For doing this, define the \textit{arrival time} \(t_a\), in function of the departure time \(t_d\):
\[t_a(t_d) = t_d + tt(t_d)\]

Suppose now that there exists a function \(tt_a(t)\) expressing the travel time in function of the arrival time:
\begin{equation}
  \label{eq:tt_a_def}
  tt_a(t_a(t_d)) = tt(t_d)
\end{equation}

In this case, the expression \eqref{eq:cost_no_alpha} considerably simplifies:
\begin{equation}
  \label{eq:cost_t_d_t_a}
  \begin{split}
    C(t_d) & = tt(t_d) + \beta [t^* - tt(t_d) - t_d]^+ + \gamma[tt(t_d) + t_d - t^*]^+ \\
           & = tt_a(t_a(t_d)) + \beta [t^* - t_a(t_d)]^+ + \gamma [t_a(t_d) - t^*]
  \end{split}
\end{equation}

Note now that the arrival time can be directly observed:
its dependency on the departure time \(t_d\) can thus be omitted,
and the cost directly expressed in function of the arrival time \(t_a\).

If~\eqref{eq:tt_a_def} is possible, \eqref{eq:cost_t_d_t_a} simply becomes
\begin{equation}
  \label{eq:cost_simplified}
  C(t_a; \beta, \gamma, t^*) = tt_a(t_a) + \beta [t^* - t_a]^+ + \gamma [t_a - t^*]^+
\end{equation}

Being this a relevant simplification of the problem,
it will be worth to study wether a function adhering to the specifications in \eqref{eq:tt_a_def} actually exists.

By expressing \eqref{eq:tt_a_def} in terms of the arrival time \(t_a\),
we get a definition of the function \(tt_a\):
\begin{equation*}
  tt_a(t_a) = tt(t_d(t_a))
\end{equation*}

The function is thus well defined as long as the function \(t_a(t_d)\) is invertible.
But this is always true with some reasonable assumptions:
if indeed the traffic is assumed to respect the \textit{First In First Out} principle
(according to which it is impossible that, by departing later, a commuter arrives earlier or at the same time),
the function \(t_a(t_d)\) is strictly increasing, and thus invertible.

More formally, we are assuming that, given \(t_1 > t_2\),
\begin{equation*}
  t_a(t_1) > t_a(t_2)
\end{equation*}

By applying the definition of the function \(t_a(t_d)\), we get
\begin{align*}
  t_1 + tt(t_1) & > t_2 + tt(t_2) \\
  t_1 - t_2 & > - tt(t_1) - (-tt(t_2))
\end{align*}
for each choice of \(t_1 > t_2\).

In particular, setting \(t_1 = t_2 + h\) yields
\begin{align*}
  - tt(t_2) - (- tt(t_2 + h)) & < h\\
  \frac{tt(t_2) - tt(t_2 + h)}{h} & > -1
\end{align*}
Finally, computing the limit for \(h \rightarrow 0\) yields a bound for the derivative of the travel time function:
\begin{equation}
  \label{eq:bound_der_tt}
  tt'(t) > -1
\end{equation}
that is, thus, a reasonable assumption.

The same reasoning can be repeated for the function \(tt_a\),
linking the arrival time to the travel time:
it will, in this case, yield the following bound for the derivative
\begin{equation}
  \label{eq:bound_der_tt_a}
  tt_a'(t) < 1
\end{equation}

These bounds will always be satisfied for realistic travel time functions,
and will have to be considered when defining theoretical travel time functions.

Please note that, on top of being necessary (as long as the travel time function is differentiable, that is not a restrictive assumption),
assumption \eqref{eq:bound_der_tt} is sufficient for defining the function \(tt_a\):
computing the derivative of the function \(t_a(t_d)\) yields indeed
\begin{align*}
  t_a'(t) & = \diff{}{t}(t + tt(t)) \\
          & = 1 + tt'(t) \\
          & > 1 - 1 = 0
\end{align*}

The function is thus strictly increasing, and so invertible.

Under some weak assumptions, the function to be minimized is thus considerably simplified:
as shown in~\eqref{eq:cost_simplified}, it is now enough to minimize the function

\begin{equation}
  \label{eq:cost_simplified_final}
  C(t_a; \beta, \gamma, t^*) = tt_a(t_a) + \beta [t^* - t_a]^+ + \gamma [t_a - t^*]^+
\end{equation}

In the following section,
the points where the minima for this function occur will be studied.

\subsection{Description of the Local Minima}
\label{sec:local_min}

The cost function, in this new form, can now be studied.
For finding the global minimum, the first step will be finding the local minima.
The local minima will then be compared, and the global minimum found.

Note now that, as long as the travel time function is differentiable,
the cost is differentiable almost everywhere as well:
the only non-differentiable operator used is indeed the operator \([\bullet ]^+\),
and \([x]^+\) is not differentiable only for \(x = 0\).
The cost function is thus differentiable in every point except for the two points
\[ t^* - t_a = 0 \qquad t_a - t^* = 0 \]
that turn out to be the same point:
\begin{equation}
  \label{eq:on_time}
  t_a = t^*
\end{equation}

The point in \eqref{eq:on_time} can thus be a local minimum.

Apart from this, the only other minima the cost function could have are the points in which its derivative is equal to zero (where the first order conditions are satisfied):
the derivative of the cost function will be

\begin{equation}
  \label{eq:der_cost_1}
  \begin{split}
    C'(t_a) & = \diff{}{t_a}(tt_a(t_a) + \beta [t^* - t_a]^+ + \gamma [t_a - t^*]^+) \\
            & = tt_a'(t_a) + \beta \diff{}{t}[t^* - t_a]^+ + \gamma \diff{}{t} [t_a - t^*]^+
  \end{split}
\end{equation}
Note now that, clearly,
\begin{equation*}
  \diff{}{t}[f(t)]^+ =
  \begin{cases}
    0 & \text{if } f(t) < 0 \\
    f'(t) & \text{if } f(t) > 0
  \end{cases}
\end{equation*}

Inserting this in \eqref{eq:der_cost_1} yields
\begin{equation}
  \label{eq:der_cost_2}
  C'(t_a) =
  \begin{cases}
    tt_a'(t_a) - \beta & \text{if } t_a < t^* \\
    tt_a'(t_a) + \gamma & \text{if } t_a > t^*
  \end{cases}
\end{equation}

By setting the derivative of the cost equal to zero,
we thus have the first order conditions:
a point \(t_a\) can be a local minimum only if it satisfies one of the following three conditions:

\begin{equation}
  \label{eq:three_minima}
  \begin{split}
    t_a & < t^*, tt_a'(t_a) = \beta \\
    t_a & = t^* \\
    t_a & > t^*, tt_a'(t_a) = -\gamma
  \end{split}
\end{equation}

I name the three minima to, respectively,
\textit{early minimum} (since the minimum is achieved before the desired arrival time),
\textit{on-time minimum} (since it is achieved when arriving exactly at the desired arrival time)
and \textit{late minimum} (since it is achieved when arriving late).

Figure~\ref{fig:cost} displays the cost function, built by utilizing a dummy
(but not too far from an analytical solution for a bottleneck model at equilibrium)
travel time function.
The function in the figure shows three local minima, each one of which is of a different kind.

\begin{figure}
  \centering
  \includegraphics[width=.8\textwidth]{cost}
  \caption{Cost function, for a travel time function defined by gluing two gaussians of different variance.
    The various minima, and their occurrence where explained in \eqref{eq:three_minima},
    are visible.
  The parameter used for this particular cost function are \(\beta = 0.6, \gamma = 1.2, t^* = 9.4\)}
  \label{fig:cost}
\end{figure}

After the local minima have been found,
where the global minimum is realized has to be computed.

\subsection{Characterization of the Global Minimum}
\label{sec:glob_min}

For characterizing the point which realizes the global minimum,
its dependency on the different parameters has to be explained.

The approach will here be incremental:
I will start by fixing the parameters \(\beta, \gamma\) and looking at how the minimum depends on the parameter \(t^*\).
After the dependency on \(t^*\) is explained, the extension to the case in which \(\beta, \gamma\) vary will be pretty natural.

It is now necessary to formalize some hypothesis on the travel time function.
I will thus recall some definitions, that will be used for constraining the travel time function.

The first definition regards a property which will be fundamental for the implementation of the model,
and that is, indeed, respected by most travel time functions (when considering only one out of the morning and evening peaks)
\begin{definition}
  Let \(f:\R \rightarrow \R\).
  
  \(f\) is said to be \textit{quasiconcave} if the preimage of any set of the form \((a, \infty)\) is convex.
\end{definition}

Please note that it is easy to prove that each quasiconcave function is unimodal (meaning that they only have one local maximum).

The next definition is a rather standard one in mathematics:
\begin{definition}
  \(C^k(\R)\) is the space of all the functions whose domain is the set of real numbers \(\R\) that are continuous, differentiable at least \(k\) times and whose \(k\)-th derivative is continuous
\end{definition}

The travel time function will be required to be quasiconcave and \(C^2(\R)\).
On top of this, it is reasonable to require it to go to zero as the time goes to infinite:
\begin{equation*}
  \lim_{t \rightarrow \infty} tt_a(t) = \lim_{t \rightarrow -\infty}tt_a(t) = 0
\end{equation*}

For simplicity, we moreover require the function to have at most a finite number of points whose derivative is equal to \(\beta\) or \(-\gamma\), for each possible value of \(\beta, \gamma\).

Lastly, I recall (from \eqref{eq:bound_der_tt_a}) that the function has to have limited growth:
\[tt_a'(t) < 1\]

It is thus now possible to define what will be, in the following pages,
the most general type of function considered:

\begin{definition}
  \label{def:gen_tt}
  Let \(f:\R \rightarrow \R\).

  f is said a \textit{general travel time function} if it satisfies the following conditions:
  \begin{enumerate}
  \item \(f \in C^2(\R)\)
  \item f is quasiconcave
  \item \(\lim_{t \rightarrow \infty} f(t) = \lim_{t \rightarrow -\infty}f(t) = 0\)
  \item \(f'(t) < 1\ \forall t\in \R\)
  \item \(|f'^{-1}(\beta)|, |f'^{-1}(-\gamma)| < \infty\ \forall \beta, \gamma > 0\)
  \end{enumerate}
\end{definition}

After this definition has been given, the point that realizes the global minimum of the cost function is realized can be characterized.
Note that, first of all,
the cost of an on-time arrival (namely, when \(t_a = t^*\)) is exactly the value of the travel time function at that time point:
\begin{equation}
  \label{eq:cost_ot}
  C(t^*; \beta, \gamma, t^*) = tt_a(t^*)
\end{equation}

It is now important to identify when arriving early or late the cost can be less than the value of the travel time.

Consider the sets of the local minima
\begin{equation}
  \label{eq:def_b_g}
  \begin{split}
    B & = \{t_a | tt_a'(t_a) = \beta, tt_a''(t_a) > 0\} \\
    G & = \{t_a | tt_a'(t_a) = \gamma, tt_a''(t_a) > 0\}
  \end{split}
\end{equation}

Given the assumptions, both sets will be finite.

Note now that, given a value for the optimal arrival time \(t^*\),
only some of these minima can be achieved.
It is thus useful to define the following sets:
\begin{align*}
  B_{t^*} & = \{t_a \in B, t_a < t^*\} \\
  G_{t^*} & = \{t_a \in G, t_a > t^*\}
\end{align*}


Consider now what happens when fixing the arrival time \(t_a\),
and varying \(t^*\):
as \(t^* = t_a\), equation \eqref{eq:cost_ot} holds:
\begin{equation*}
  C(t^*; \beta, \gamma, t^*) = tt(t^*)
\end{equation*}

Consider now increasing \(t^*\):
let
\[t^* = t_a + \varepsilon,\ \varepsilon \geq 0\]

The cost will become, from its definition in \eqref{eq:cost_simplified_final},
\begin{align*}
  C(t_a; \beta, \gamma, t_a+\varepsilon) & = tt(t_a) + \beta [t_a + \varepsilon - t_a]^+ + \gamma [t_a - t_a - \varepsilon]^+ \\
  & = tt(t_a) + \beta\varepsilon \tag{\theequation}\label{eq:linear_increase_cost}
\end{align*}

When increasing \(t^*\), the cost of arriving at time \(t_a\) is thus linearly increasing on the distance between \(t^*\) and \(t_a\),
with coefficient \(\beta\).

Similarly, consider decreasing \(t^*\):
\[t^* = t_a - \varepsilon,\ \varepsilon \geq 0\]

The cost will now be, analogously to \eqref{eq:linear_increase_cost},
\begin{equation}
  \label{eq:linear_increase_cost}
  C(t_a; \beta, \gamma, t_a - \varepsilon) = tt(t_a) + \gamma\varepsilon
\end{equation}

Again, and similarly to \eqref{eq:linear_increase_cost},
we see that when decreasing the optimal arrival time towards the actual arrival time \(t_a\),
the cost of arriving at a fixed time increases linearly,
with coefficient \(\gamma\).

The following theorem shows that the minimum \(t^{opt}\) is realized by the point that,
among the ones in \(B_{t^*}, G_{t^*}\),
satisfy a simple condition:
\begin{theorem}
    Let \(tt_a(t_a)\) be a general travel time function, as in definition \ref{def:gen_tt},
  and let (in increasing order) \(B = \{t^\beta_1, \dots, t^\beta_n\}\), \(G = \{t^\gamma_1, \dots, t^\gamma_m\}\).


  Consider the collections of intervals \(B^* = \{(t^\beta_i, b_e(t^\beta_i))\}_i\) and \(G^* = \{(g_e(t^\gamma_i), t^\gamma_i)\}_i\), where

  \begin{align*}
    b_e(t_a) & = \min\{t | tt(t) = tt(t_a) + \beta(t - t_a), t > t_a\} \\
    g_i(t_a) & = \max\{t | tt(t) = tt(t_a) - \gamma(t - t_a), t < t_a\} \\
  \end{align*}
  
  if \(\exists i, j |b_e(t^\beta_i) < g_i(t^\gamma_j)\), then the function \(t^{opt}(t^*)\) will be as follows:

  \begin{equation}
    \label{char_opt}
    t^{opt}(t^*) = 
    \begin{cases}
      \min_i \{t^\beta_i | t^* \in (t^\beta_i, b_e(t^\beta_i))\} & \text{if } \exists i |\ t^\beta_i < t^* < b_e(t^\beta_i) \\
      \max_i \{t^\gamma_i | t^* \in (g_i(t^\gamma_i), t^\gamma_i)\} & \text{if } \exists i |\ g_i(t^\gamma_i) < t^* < t^\gamma_i \\
      t^* & \text{otherwise}
    \end{cases}
  \end{equation}

  Otherwise, let
  \begin{align*}
    t^\beta_{max} & = \argmax_{t \in B}b_e(t) \\
    t^\gamma_{min} & = \argmin_{t \in G}g_i(t)
  \end{align*}

  For \(t^* \notin (t^\beta_{max}, t^\gamma_{min})\), the characterization \eqref{char_opt} holds.

  For \(t^* \in (t^\beta_{max}, t^\gamma_{min})\), the function will be characterized by the following:
  \begin{equation}
    \label{eq:char_strange}
    t^{opt}(t^*) =
    \begin{cases}
      t^\beta_{max} & \text{if } tt(t^\beta_{max}) + \beta(t^* - t^\beta_{max}) < tt(t^\gamma_{min}) - \gamma(t^* - t^\gamma_{min}) \\
      t^\gamma_{min} & \text{otherwise}
    \end{cases}
  \end{equation}
\end{theorem}

\begin{figure}
  \centering
  \includegraphics{comptt}
  \caption{An arbitrary travel time function, with the possible minimizers for the cost.
    All the red and green solid lines represent achievable costs,
    by realizing an early or late arrival at the time highlighted by the dashed line.
  Minimizing all the plotted functions yields a graphical representation of the cost as a function of the optimal arrival time \(t^*\).}
  \label{fig:complex_tt}
\end{figure}

The proof of the theorem is omitted.
The intuition around it can anyway be built by looking at figure~\ref{fig:complex_tt}:
by arriving early or late, the user can achieve a linear growth of the cost.
This makes these types of arrivals convenient as long as the travel time function grows superlinearly,
that makes the cost of an on-time arrival growing accordingly.

Once this general result is shown, for the rest of the thesis the analysis will be restricted to a more specific travel time function,
which will be called \textit{proper travel time function}:
\begin{definition}
  \label{def:proper_tt}
  Let \(f:\R\rightarrow\R\).

  f is a \textit{proper travel time function} if \(f\) is a general travel time function and,
  on top of this, 
  two points \(k_1\neq k_2 \in \R\) can be found such that
  \begin{itemize}
  \item \(f|_{(k_1, k_2)}\) is concave
  \item \(f|_{\R\setminus(k_1, k_2)}\) is convex
  \end{itemize}
\end{definition}

Please note how this is not a highly restrictive requirement,
when considering real-world scenarios:
it is indeed exactly what one would expect from a travel time function,
and the analytical solutions to Vickrey's model,
including (especially) the ones with heterogeneous population \parencite{amirgholy2017analytical}.

It is, anyway, a property that considerably simplifies the estimation work:
by restricting the possible functions in this way,
the derivative of them can only grow once, decrease and then stabilize to zero,
as shown in figure~\ref{fig:prop_travel_time}.
This property makes the definition particularly useful, since it makes the following true:

\begin{figure}
  \centering
  \includegraphics[width=.7\textwidth]{tt_proper}
  \caption{Example of a well-behaved proper travel time function.
    The behaviour of the derivative is the most complex one allowed:
    it increases from zero, decreases and eventually again increases to reach zero.
  This behaviour considerably simplifies the estimation, as noted in proposition~\ref{prop:unique_min}}
  \label{fig:prop_travel_time}
\end{figure}

\begin{prop}
  \label{prop:unique_min}
  Let \(tt_a:\R\rightarrow\R\) be a proper travel time function, \(\beta, \gamma \in \R^+\).

  The sets \(B, G\) (as defined in \eqref{eq:def_b_g}) will consist of at most one element.
\end{prop}

\begin{proof}
  Let \(k_1, k_2\) be defined as in definition~\ref{def:proper_tt}.

  First of all, note that, for \(t\in[k_1, k_2]\), the function \(tt_a(t)\) is convex and satisfies thus
  \[tt_a''(t) < 0\]

  Since in~\eqref{eq:def_b_g} the second derivative is required to be positive,
  no points in the interval \([k_1, k_2]\) are in the sets \(B, G\):
  \begin{equation*}
    B\cap[k_1, k_2] = G\cap[k_1, k_2] = \emptyset
  \end{equation*}

  The sets \(B, G\) will thus be completely defined by the points in \(\R\setminus[k_1, k_2]\).

  Consider now the points \(t\in(-\infty, k_1)\).
  The function is here concave, and the derivative is thus monotonous.
  Moreover, the hypotheses (namely, the unimodality) imply the function to be there increasing:
  the derivative is thus positive.

  The derivative is thus an injective function in the set \(\R^+ = (0, \infty)\):
  \begin{equation*}
    tt_a'|_{(-\infty, k_1)}:(-\infty, k_1) \xhookrightarrow{} \R^+
  \end{equation*}

  Being the function injective, the preimage of any value consists of at most one point.
  By considering the function only on \((-\infty, k_1)\),
  the set \(B\) is made of at most one point,
  while the set \(G\) is empty (since \(-\gamma < 0\)).

  A similar reasoning shows that, when restricting the function to the set \((k_2, \infty)\),
  the result is injective on the set \(\R^- = (-\infty, 0)\):
  \begin{equation*}
    tt_a'|_{(k_2, \infty)}:(k_2, \infty) \hookrightarrow{} \R^-
  \end{equation*}

  Considering thus the restriction of \(tt_a\) to the interval \((k_2, \infty)\),
  the set \(G\) is made of at most one point,
  while \(B\) is empty (since \(\beta > 0\)).

  Considering thus the function on all its domain (that is, \(\R\)),
  the sets \(B, G\) will both be made of at most one point.

\end{proof}




%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
