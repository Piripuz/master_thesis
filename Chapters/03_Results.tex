\chapter{Results}
\label{chap:res}

In this chapter, the results of the research will be presented.

The first step for building the model is some analytical work:
in section~\ref{sec:minimum},the points in which the cost function is minimized are studied,
and a characterization of the minima is done.
This characterization is then be used in section \ref{sec:lik},
where the statistical framework is developed and the likelihood function introduced.

Once an expression for the likelihood function is found,
it can be numerically implemented:
details of the implementation, and its results on the various test types,
are shown in the following sections.

\section{Minimizing the Cost Function}
\label{sec:minimum}

For characterizing where the cost function is minimized,
it is a good idea to simplify it as much as possible.
Some theoretical preliminaries are hence necessary.

\subsection{Theoretical Preliminaries}
\label{sec:pre_minimizing}

Recall that, from equation \eqref{eq:cost_inf}, we are assuming the cost to be

\begin{equation*}
  C(t; \alpha, \beta, \gamma, t^*) = \alpha tt(t) + \beta [t^* - tt(t) - t]^+ + \gamma[tt(t) + t - t^*]^+
\end{equation*}

where

\begin{itemize}
\item \(t^*\) is the desired arrival time
\item \(\alpha\) is the value of time spent travelling
\item \(\beta\) is the value of time spent waiting at the destination
\item \(\gamma\) is the value of time arriving late at the destination
\item \(tt(t_d)\) is the time spent travelling if departing at time \(t_d\)
\item \([x]^+ = \max(0, x)\)
\end{itemize}

The first observation is that minimizing a function is equivalent to minimizing a scaled version of it:
instead of minimizing the function \(C(t_d)\),
I will study the minimization problem on the function \(C(t_d)/\alpha\) or,
equivalently, normalize the parameter \(\alpha\) to 1.

From now on a slightly different cost function will be considered:
\begin{equation}
  \label{eq:cost_no_alpha}
  C(t; \beta, \gamma, t^*) = tt(t) + \beta [t^* - tt(t) - t]^+ + \gamma[tt(t) + t - t^*]^+
\end{equation}
where the numerical values of the parameters \(\beta, \gamma\) are actually representing the values for \(\beta/\alpha, \gamma/\alpha\).

Note now that most of the cost function depends on when the user \textit{decides to arrive} rather than when the user decides to leave.
Expressing it in terms of the arrival time may thus be more natural,
and simplify the expression of the cost function.

For doing this, define the \textit{arrival time} \(t_a\), in function of the departure time \(t_d\):
\[t_a(t_d) = t_d + tt(t_d)\]

Suppose now that there exists a function \(tt_a(t)\) expressing the travel time in function of the arrival time:
\begin{equation}
  \label{eq:tt_a_def}
  tt_a(t_a(t_d)) = tt(t_d)
\end{equation}

In this case, the expression \eqref{eq:cost_no_alpha} considerably simplifies:
\begin{equation}
  \label{eq:cost_t_d_t_a}
  \begin{split}
    C(t_d) & = tt(t_d) + \beta [t^* - tt(t_d) - t_d]^+ + \gamma[tt(t_d) + t_d - t^*]^+ \\
           & = tt_a(t_a(t_d)) + \beta [t^* - t_a(t_d)]^+ + \gamma [t_a(t_d) - t^*]
  \end{split}
\end{equation}

Note now that the arrival time can be directly observed:
its dependency on the departure time \(t_d\) can thus be omitted,
and the cost directly expressed in function of the arrival time \(t_a\).

If~\eqref{eq:tt_a_def} is possible, \eqref{eq:cost_t_d_t_a} simply becomes
\begin{equation}
  \label{eq:cost_simplified}
  C(t_a; \beta, \gamma, t^*) = tt_a(t_a) + \beta [t^* - t_a]^+ + \gamma [t_a - t^*]^+
\end{equation}

Being this a relevant simplification of the problem,
it will be worth to study wether a function adhering to the specifications in \eqref{eq:tt_a_def} actually exists.

By expressing \eqref{eq:tt_a_def} in terms of the arrival time \(t_a\),
we get a definition of the function \(tt_a\):
\begin{equation*}
  tt_a(t_a) = tt(t_d(t_a))
\end{equation*}

The function is thus well defined as long as the function \(t_a(t_d)\) is invertible.
But this is always true with some reasonable assumptions:
if indeed the traffic is assumed to respect the \textit{First In First Out} principle
(according to which it is impossible that, by departing later, a commuter arrives earlier or at the same time),
the function \(t_a(t_d)\) is strictly increasing, and thus invertible.

More formally, we are assuming that, given \(t_1 > t_2\),
\begin{equation*}
  t_a(t_1) > t_a(t_2)
\end{equation*}

By applying the definition of the function \(t_a(t_d)\), we get
\begin{align*}
  t_1 + tt(t_1) & > t_2 + tt(t_2) \\
  t_1 - t_2 & > - tt(t_1) - (-tt(t_2))
\end{align*}
for each choice of \(t_1 > t_2\).

In particular, setting \(t_1 = t_2 + h\) yields
\begin{align*}
  - tt(t_2) - (- tt(t_2 + h)) & < h\\
  \frac{tt(t_2) - tt(t_2 + h)}{h} & > -1
\end{align*}
Finally, computing the limit for \(h \rightarrow 0\) yields a bound for the derivative of the travel time function:
\begin{equation}
  \label{eq:bound_der_tt}
  tt'(t) > -1
\end{equation}
that is, thus, a reasonable assumption.

The same reasoning can be repeated for the function \(tt_a\),
linking the arrival time to the travel time:
it will, in this case, yield the following bound for the derivative
\begin{equation}
  \label{eq:bound_der_tt_a}
  tt_a'(t) < 1
\end{equation}

These bounds will always be satisfied for realistic travel time functions,
and will have to be considered when defining theoretical travel time functions.

Please note that, on top of being necessary (as long as the travel time function is differentiable, that is not a restrictive assumption),
assumption \eqref{eq:bound_der_tt} is sufficient for defining the function \(tt_a\):
computing the derivative of the function \(t_a(t_d)\) yields indeed
\begin{align*}
  t_a'(t) & = \diff{}{t}(t + tt(t)) \\
          & = 1 + tt'(t) \\
          & > 1 - 1 = 0
\end{align*}

The function is thus strictly increasing, and so invertible.

Under some weak assumptions, the function to be minimized is thus considerably simplified:
as shown in~\eqref{eq:cost_simplified}, it is now enough to minimize the function

\begin{equation}
  \label{eq:cost_simplified_final}
  C(t_a; \beta, \gamma, t^*) = tt_a(t_a) + \beta [t^* - t_a]^+ + \gamma [t_a - t^*]^+
\end{equation}

In the following section,
the points where the minima for this function occur will be studied.

\subsection{Description of the Local Minima}
\label{sec:local_min}

The cost function, in this new form, can now be studied.
For finding the global minimum, the first step will be finding the local minima.
The local minima will then be compared, and the global minimum found.

Note now that, as long as the travel time function is differentiable,
the cost is differentiable almost everywhere as well:
the only non-differentiable operator used is indeed the operator \([\bullet ]^+\),
and \([x]^+\) is not differentiable only for \(x = 0\).
The cost function is thus differentiable in every point except for the two points
\[ t^* - t_a = 0 \qquad t_a - t^* = 0 \]
that turn out to be the same point:
\begin{equation}
  \label{eq:on_time}
  t_a = t^*
\end{equation}

The point in \eqref{eq:on_time} can thus be a local minimum.

Apart from this, the only other minima the cost function could have are the points in which its derivative is equal to zero (where the first order conditions are satisfied):
the derivative of the cost function will be

\begin{equation}
  \label{eq:der_cost_1}
  \begin{split}
    C'(t_a) & = \diff{}{t_a}(tt_a(t_a) + \beta [t^* - t_a]^+ + \gamma [t_a - t^*]^+) \\
            & = tt_a'(t_a) + \beta \diff{}{t}[t^* - t_a]^+ + \gamma \diff{}{t} [t_a - t^*]^+
  \end{split}
\end{equation}
Note now that, clearly,
\begin{equation*}
  \diff{}{t}[f(t)]^+ =
  \begin{cases}
    0 & \text{if } f(t) < 0 \\
    f'(t) & \text{if } f(t) > 0
  \end{cases}
\end{equation*}

Inserting this in \eqref{eq:der_cost_1} yields
\begin{equation}
  \label{eq:der_cost_2}
  C'(t_a) =
  \begin{cases}
    tt_a'(t_a) - \beta & \text{if } t_a < t^* \\
    tt_a'(t_a) + \gamma & \text{if } t_a > t^*
  \end{cases}
\end{equation}

By setting the derivative of the cost equal to zero,
we thus have the first order conditions:
a point \(t_a\) can be a local minimum only if it satisfies one of the following three conditions:

\begin{equation}
  \label{eq:three_minima}
  \begin{split}
    t_a & < t^*, tt_a'(t_a) = \beta \\
    t_a & = t^* \\
    t_a & > t^*, tt_a'(t_a) = -\gamma
  \end{split}
\end{equation}

I name the three minima to, respectively,
\textit{early minimum} (since the minimum is achieved before the desired arrival time),
\textit{on-time minimum} (since it is achieved when arriving exactly at the desired arrival time)
and \textit{late minimum} (since it is achieved when arriving late).

Figure~\ref{fig:cost} displays the cost function, built by utilizing a dummy
(but not too far from an analytical solution for a bottleneck model at equilibrium)
travel time function.
The function in the figure shows three local minima, each one of which is of a different kind.

\begin{figure}
  \centering
  \includegraphics[width=.8\textwidth]{cost}
  \caption{Cost function, for a travel time function defined by gluing two gaussians of different variance.
    The various minima, and their occurrence where explained in \eqref{eq:three_minima},
    are visible.
  The parameter used for this particular cost function are \(\beta = 0.6, \gamma = 1.2, t^* = 9.4\)}
  \label{fig:cost}
\end{figure}

After the local minima have been found,
where the global minimum is realized has to be computed.

\subsection{Characterization of the Global Minimum}
\label{sec:glob_min}

For characterizing the point which realizes the global minimum,
its dependency on the different parameters has to be explained.

The approach will here be incremental:
I will start by fixing the parameters \(\beta, \gamma\) and looking at how the minimum depends on the parameter \(t^*\).
After the dependency on \(t^*\) is explained, the extension to the case in which \(\beta, \gamma\) vary will be pretty natural.

It is now necessary to formalize some hypothesis on the travel time function.
I will thus recall some definitions, that will be used for constraining the travel time function.

The first definition regards a property which will be fundamental for the implementation of the model,
and that is, indeed, respected by most travel time functions (when considering only one out of the morning and evening peaks)
\begin{definition}
  Let \(f:\R \rightarrow \R\).
  
  \(f\) is said to be \textit{quasiconcave} if the preimage of any set of the form \((a, \infty)\) is convex.
\end{definition}

Please note that it is easy to prove that each quasiconcave function is unimodal (meaning that they only have one local maximum).

The next definition is a rather standard one in mathematics:
\begin{definition}
  \(C^k(\R)\) is the space of all the functions whose domain is the set of real numbers \(\R\) that are continuous, differentiable at least \(k\) times and whose \(k\)-th derivative is continuous
\end{definition}

The travel time function will be required to be quasiconcave and \(C^2(\R)\).
On top of this, it is reasonable to require it to go to zero as the time goes to infinite:
\begin{equation*}
  \lim_{t \rightarrow \infty} tt_a(t) = \lim_{t \rightarrow -\infty}tt_a(t) = 0
\end{equation*}

For simplicity, we moreover require the function to have at most a finite number of points whose derivative is equal to \(\beta\) or \(-\gamma\), for each possible value of \(\beta, \gamma\).

Lastly, I recall (from \eqref{eq:bound_der_tt_a}) that the function has to have limited growth:
\[tt_a'(t) < 1\]

It is thus now possible to define what will be, in the following pages,
the most general type of function considered:

\begin{definition}
  \label{def:gen_tt}
  Let \(f:\R \rightarrow \R\).

  f is said a \textit{general travel time function} if it satisfies the following conditions:
  \begin{enumerate}
  \item \(f \in C^2(\R)\)
  \item f is quasiconcave
  \item \(\lim_{t \rightarrow \infty} f(t) = \lim_{t \rightarrow -\infty}f(t) = 0\)
  \item \(f'(t) < 1\ \forall t\in \R\)
  \item \(|f'^{-1}(\beta)|, |f'^{-1}(-\gamma)| < \infty\ \forall \beta, \gamma > 0\)
  \end{enumerate}
\end{definition}

After this definition has been given, the point that realizes the global minimum of the cost function is realized can be characterized.
Note that, first of all,
the cost of an on-time arrival (namely, when \(t_a = t^*\)) is exactly the value of the travel time function at that time point:
\begin{equation}
  \label{eq:cost_ot}
  C(t^*; \beta, \gamma, t^*) = tt_a(t^*)
\end{equation}

It is now important to identify when arriving early or late the cost can be less than the value of the travel time.

Consider the sets of the local minima
\begin{equation}
  \label{eq:def_b_g}
  \begin{split}
    B & = \{t_a | tt_a'(t_a) = \beta, tt_a''(t_a) > 0\} \\
    G & = \{t_a | tt_a'(t_a) = \gamma, tt_a''(t_a) > 0\}
  \end{split}
\end{equation}

Given the assumptions, both sets will be finite.

Note now that, given a value for the optimal arrival time \(t^*\),
only some of these minima can be achieved.
It is thus useful to define the following sets:
\begin{align*}
  B_{t^*} & = \{t_a \in B, t_a < t^*\} \\
  G_{t^*} & = \{t_a \in G, t_a > t^*\}
\end{align*}


Consider now what happens when fixing the arrival time \(t_a\),
and varying \(t^*\):
as \(t^* = t_a\), equation \eqref{eq:cost_ot} holds:
\begin{equation*}
  C(t^*; \beta, \gamma, t^*) = tt(t^*)
\end{equation*}

Consider now increasing \(t^*\):
let
\[t^* = t_a + \varepsilon,\ \varepsilon \geq 0\]

The cost will become, from its definition in \eqref{eq:cost_simplified_final},
\begin{align*}
  C(t_a; \beta, \gamma, t_a+\varepsilon) & = tt(t_a) + \beta [t_a + \varepsilon - t_a]^+ + \gamma [t_a - t_a - \varepsilon]^+ \\
  & = tt(t_a) + \beta\varepsilon \tag{\theequation}\label{eq:linear_increase_cost}
\end{align*}

When increasing \(t^*\), the cost of arriving at time \(t_a\) is thus linearly increasing on the distance between \(t^*\) and \(t_a\),
with coefficient \(\beta\).

Similarly, consider decreasing \(t^*\):
\[t^* = t_a - \varepsilon,\ \varepsilon \geq 0\]

The cost will now be, analogously to \eqref{eq:linear_increase_cost},
\begin{equation}
  \label{eq:linear_increase_cost}
  C(t_a; \beta, \gamma, t_a - \varepsilon) = tt(t_a) + \gamma\varepsilon
\end{equation}

Again, and similarly to \eqref{eq:linear_increase_cost},
we see that when decreasing the optimal arrival time towards the actual arrival time \(t_a\),
the cost of arriving at a fixed time increases linearly,
with coefficient \(\gamma\).

The following theorem shows that the minimum \(t^{opt}\) is realized by the point that,
among the ones in \(B_{t^*}, G_{t^*}\),
satisfy a simple condition:
\begin{theorem}
  \label{theo:characterization}
    Let \(tt_a(t_a)\) be a general travel time function, as in definition \ref{def:gen_tt},
  and let (in increasing order) \(B = \{t^\beta_1, \dots, t^\beta_n\}\), \(G = \{t^\gamma_1, \dots, t^\gamma_m\}\).


  Consider the collections of intervals \(B^* = \{(t^\beta_i, b_e(t^\beta_i))\}_i\) and \(G^* = \{(g_e(t^\gamma_i), t^\gamma_i)\}_i\), where

  \begin{align*}
    b_e(t_a) & = \min\{t | tt(t) = tt(t_a) + \beta(t - t_a), t > t_a\} \\
    g_i(t_a) & = \max\{t | tt(t) = tt(t_a) - \gamma(t - t_a), t < t_a\}
  \end{align*}
  
  if \(\exists i, j |b_e(t^\beta_i) < g_i(t^\gamma_j)\), then the function \(t^{opt}(t^*)\) will be as follows:

  \begin{equation}
    \label{char_opt}
    t^{opt}(t^*) = 
    \begin{cases}
      \min_i \{t^\beta_i | t^* \in (t^\beta_i, b_e(t^\beta_i))\} & \text{if } \exists i |\ t^\beta_i < t^* < b_e(t^\beta_i) \\
      \max_i \{t^\gamma_i | t^* \in (g_i(t^\gamma_i), t^\gamma_i)\} & \text{if } \exists i |\ g_i(t^\gamma_i) < t^* < t^\gamma_i \\
      t^* & \text{otherwise}
    \end{cases}
  \end{equation}

  Otherwise, let
  \begin{align*}
    t^\beta_{max} & = \argmax_{t \in B}b_e(t) \\
    t^\gamma_{min} & = \argmin_{t \in G}g_i(t)
  \end{align*}

  For \(t^* \notin (t^\beta_{max}, t^\gamma_{min})\), the characterization \eqref{char_opt} holds.

  For \(t^* \in (t^\beta_{max}, t^\gamma_{min})\), the function will be characterized by the following:
  \begin{equation}
    \label{eq:char_strange}
    t^{opt}(t^*) =
    \begin{cases}
      t^\beta_{max} & \text{if } tt(t^\beta_{max}) + \beta(t^* - t^\beta_{max}) < tt(t^\gamma_{min}) - \gamma(t^* - t^\gamma_{min}) \\
      t^\gamma_{min} & \text{otherwise}
    \end{cases}
  \end{equation}
\end{theorem}

\begin{figure}
  \centering
  \includegraphics{comptt}
  \caption{An arbitrary travel time function, with the possible minimizers for the cost.
    All the red and green solid lines represent achievable costs,
    by realizing an early or late arrival at the time highlighted by the dashed line.
  Minimizing all the plotted functions yields a graphical representation of the cost as a function of the optimal arrival time \(t^*\).}
  \label{fig:complex_tt}
\end{figure}

The proof of the theorem is omitted.
The intuition around it can anyway be built by looking at figure~\ref{fig:complex_tt}:
by arriving early or late, the user can achieve a linear growth of the cost.
This makes these types of arrivals convenient as long as the travel time function grows superlinearly,
that makes the cost of an on-time arrival growing accordingly.

Once this general result is shown, for the rest of the thesis the analysis will be restricted to a more specific travel time function,
which will be called \textit{proper travel time function}:
\begin{definition}
  \label{def:proper_tt}
  Let \(f:\R\rightarrow\R\).

  f is a \textit{proper travel time function} if \(f\) is a general travel time function and,
  on top of this, 
  two points \(k_1\neq k_2 \in \R\) can be found such that
  \begin{itemize}
  \item \(f|_{(k_1, k_2)}\) is concave
  \item \(f|_{\R\setminus(k_1, k_2)}\) is convex
  \end{itemize}
\end{definition}

Please note how this is not a highly restrictive requirement,
when considering real-world scenarios:
it is indeed exactly what one would expect from a travel time function,
and the analytical solutions to Vickrey's model,
including (especially) the ones with heterogeneous population \parencite{amirgholy2017analytical}.

It is, anyway, a property that considerably simplifies the estimation work:
by restricting the possible functions in this way,
the derivative of them can only grow once, decrease and then stabilize to zero,
as shown in figure~\ref{fig:prop_travel_time}.
This property makes the definition particularly useful, since it makes the following true:

\begin{figure}
  \centering
  \includegraphics[width=.7\textwidth]{tt_proper}
  \caption{Example of a well-behaved proper travel time function.
    The behaviour of the derivative is the most complex one allowed:
    it increases from zero, decreases and eventually again increases to reach zero.
  This behaviour considerably simplifies the estimation, as noted in proposition~\ref{prop:unique_min}}
  \label{fig:prop_travel_time}
\end{figure}

\begin{prop}
  \label{prop:unique_min}
  Let \(tt_a:\R\rightarrow\R\) be a proper travel time function, \(\beta, \gamma \in \R^+\).

  The sets \(B, G\) (as defined in \eqref{eq:def_b_g}) will consist of at most one element.
\end{prop}

\begin{proof}
  Let \(k_1, k_2\) be defined as in definition~\ref{def:proper_tt}.

  First of all, note that, for \(t\in[k_1, k_2]\), the function \(tt_a(t)\) is convex and satisfies thus
  \[tt_a''(t) < 0\]

  Since in~\eqref{eq:def_b_g} the second derivative is required to be positive,
  no points in the interval \([k_1, k_2]\) are in the sets \(B, G\):
  \begin{equation*}
    B\cap[k_1, k_2] = G\cap[k_1, k_2] = \emptyset
  \end{equation*}

  The sets \(B, G\) will thus be completely defined by the points in \(\R\setminus[k_1, k_2]\).

  Consider now the points \(t\in(-\infty, k_1)\).
  The function is here concave, and the derivative is thus monotonous.
  Moreover, the hypotheses (namely, the unimodality) imply the function to be there increasing:
  the derivative is thus positive.

  The derivative is thus an injective function in the set \(\R^+ = (0, \infty)\):
  \begin{equation*}
    tt_a'|_{(-\infty, k_1)}:(-\infty, k_1) \xhookrightarrow{} \R^+
  \end{equation*}

  Being the function injective, the preimage of any value consists of at most one point.
  By considering the function only on \((-\infty, k_1)\),
  the set \(B\) is made of at most one point,
  while the set \(G\) is empty (since \(-\gamma < 0\)).

  A similar reasoning shows that, when restricting the function to the set \((k_2, \infty)\),
  the result is injective on the set \(\R^- = (-\infty, 0)\):
  \begin{equation*}
    tt_a'|_{(k_2, \infty)}:(k_2, \infty) \hookrightarrow{} \R^-
  \end{equation*}

  Considering thus the restriction of \(tt_a\) to the interval \((k_2, \infty)\),
  the set \(G\) is made of at most one point,
  while \(B\) is empty (since \(\beta > 0\)).

  Considering thus the function on all its domain (that is, \(\R\)),
  the sets \(B, G\) will both be made of at most one point.

\end{proof}

From now on, these points will be denoted as

\begin{equation}
  \label{eq:t_beta_t_gamma}
  B = \{t^\beta\}\qquad G = \{t^\gamma\}
\end{equation}

The cases in which at least one of the sets is empty will naturally follow from the one in which none of them is,
since are only simpler.

If the function is a proper travel time function, the following functions are thus well defined:

\begin{equation}
  \label{eq:def_b_i}
  \begin{split}
    b_i:(0, \max_t tt_a'(t)]&\rightarrow \R \\
    \beta & \mapsto t^\beta
  \end{split}
\end{equation}
and similarly, for \(\gamma\),

\begin{equation}
  \label{eq:def_g_e}
  \begin{split}
    g_e:(0, -\min_t tt_a'(t)]&\rightarrow \R \\
    \gamma & \mapsto t^\gamma
  \end{split}
\end{equation}

Define now, similarly to what done in theorem~\ref{theo:characterization}, the functions \(b_e, g_i\):
\begin{equation}
  \label{eq:def_b_e_g_i}
  \begin{split}
    b_e(\beta) & = t | tt_a(t) = \beta (t - b_i(\beta)) + tt_a(b_i(\beta)),\ t > b_i(\beta) \\
    g_i(\gamma) & = t | tt_a(t) = \gamma (g_e(\gamma) - t) + tt_a(g_e(\gamma)),\ t < g_e(\gamma)
  \end{split}
\end{equation}

\begin{figure}
  \centering
  \includegraphics[width=.8\textwidth]{tt_functions}
  \caption{A theoretical proper travel time function,
    drawn with the tangent lines corresponding to the optimal late and early arrival.
    The four functions \(b_i, b_e, g_i\) and \(g_e\) are shown:
  they locate the points in which an early (or late) arrival becomes (or ends to be) profitable.}
  \label{fig:tt_functions}
\end{figure}

Note that \(b_e, g_i\) are well defined as long as \(tt_a\) is a proper travel time function.
Figure~\ref{fig:tt_functions} shows the purpose of the defined functions:
an early or late arrival is convenient (and will thus be realized) if and only if the desired arrival time \(t^*\) falls in the interval described by the functions:
theorem~\ref{theo:characterization} reduces thus, in this case, to the following observation:
\begin{obs}
  \label{obs:char_opt_time_simple}
  If \(tt_a\) is a proper travel time function, and \(\beta, \gamma\) are such that \(b_e(\beta) < g_i(\gamma)\), the optimal arrival time \(t^{opt}\) is characterized as follows:
  \begin{equation}
    \label{eq:char_opt_time_simple}
    t^{opt} =
    \begin{cases}
      b_i(\beta) & \text{if } t^*\in(b_i(\beta), b_e(\beta)) \\
      g_e(\gamma) & \text{if } t^*\in(g_i(\gamma), g_e(\gamma)) \\
      t^* & \text{otherwise}
    \end{cases}
  \end{equation}
\end{obs}

This is a simple consequence of theorem~\ref{theo:characterization}.

In the case in which the costs of arriving early and late intersect \textit{below} the travel time function
(that is, when \(b_e(\beta) > g_i(\gamma)\)),
the situation is slightly more complex:
for tackling this case, it is necessary to determine the point in which they intersect.

Let thus \(t_s(\beta, \gamma)\) be the point corresponding to a desired arrival time \(t^*\) for which arriving late or early makes no difference.
This point will be interesting when its cost will be smaller than the cost of arriving on time (since, when the cost of arriving on time will be smaller,
we can just forget the position of this point and yield an on-time arrival for every point in a \label{eq:char_opt_time_comp}
neighbourhood of it).
In this case, we have three points \(b_e(\beta), g_i(\gamma)\) and \(t_s(\beta, \gamma)\),
and the only allowed orderings (in the case in which at least two of them are distinct) for them are two:
either
\begin{equation}\label{eq:simple_ordering}
  b_e(\beta) < t_s(\beta, \gamma) < g_i(\gamma)
\end{equation}
or
\begin{equation}\label{eq:complex_ordering}
  b_e(\beta) > t_s(\beta, \gamma) > g_i(\gamma)
\end{equation}

The proof of this is trivial, and is thus omitted.

In case \eqref{eq:simple_ordering}, the assumption \(b_e(\beta) < g_i(\gamma)\) clearly holds, and there's nothing more to discuss.

Otherwise, minimizing the cost function will yield an early arrival until the point \(t_s(\beta, \gamma)\), and a late arrival after that point.
The following observation would thus hold:
\begin{obs}
  \label{obs:char_opt_time_comp}
  If \(tt_a\) is a proper travel time function, and \(\beta, \gamma\) are such that \(b_e(\beta) > g_i(\gamma)\), the optimal arrival time \(t^{opt}\) is characterized as follows:
  \begin{equation}
    t^{opt} =
    \begin{cases}
      b_i(\beta) & \text{if } t^*\in(b_i(\beta), t_s(\beta, \gamma)) \\
      g_e(\gamma) & \text{if } t^*\in(t_s(\beta, \gamma), g_e(\gamma)) \\
      t^* & \text{otherwise}
    \end{cases}
  \end{equation}
\end{obs}

\begin{figure}
  \centering
  \includegraphics[width=.8\textwidth]{tt_functions_int}
  \caption{A theoretical proper travel time functions, with lines corresponding to optimal early and late arrivals.
    Here, the hypothesis in observation~\ref{obs:char_opt_time_simple} are not satisfied:
    the point \(t_s(\beta, \gamma)\) has thus to be studied,
  and its position is depicted in the plot.}
  \label{fig:tt_functions_int}
\end{figure}

In other words, no on-time arrival is realized during the traffic peak.
Figure~\ref{fig:tt_functions_int} shows this case, in which the location of the point \(t_s(\beta, \gamma)\) has to be computed

It is thus useful to, given \(\beta\), \(\gamma\), find an expression for the point \(t_s\).

The line representing the cost of arriving early, in function of \(t^*\), is
\begin{equation*}
  C_b(t) = (t - b_i(\beta))\beta + tt_a(b_i(\beta))
\end{equation*}
similarly, arriving late costs
\begin{equation*}
  C_g(t) = -(t - g_e(\gamma))\gamma + tt_a(g_e(\gamma))
\end{equation*}

The point in which arriving late costs as much as arriving early is thus the one that realizes
\begin{equation}
  \label{eq:t_s_numbers}
  \begin{split}
    C_b(t_s) & = C_g(t_s) \\
    (t_s - b_i(\beta))\beta + tt_a(b_i(\beta)) & = -(t_s - g_e(\gamma))\gamma + tt_a(g_e(\gamma)) \\
    (\beta + \gamma)t_s & = \beta b_i(\beta) - tt_a(b_i(\beta)) + \gamma g_e(\gamma) + tt_a(g_e(\gamma)) \\
    t_s(\beta, \gamma) & = \frac{\beta b_i(\beta) - tt_a(b_i(\beta)) + \gamma g_e(\gamma) + tt_a(g_e(\gamma))}{(\beta + \gamma)}
  \end{split}
\end{equation}

Even being this expression convoluted, it is an analytical expression for the function \(t_s\).
This conclude the section, by fully characterizing the function \(t^{opt}(t^*, \beta, \gamma)\).

The characterization follows by observations~\ref{obs:char_opt_time_simple} and~\ref{obs:char_opt_time_comp}:
it can thus be computed as follows:
\begin{enumerate}
\item Given \(\beta, \gamma\), the four points \(b_i(\beta), b_e(\beta), g_i(\gamma), g_e(\gamma)\) are found, by using their definitions:
  \begin{equation}
    \label{eq:def_b_ieg_ie}
    \begin{split}
      b_i(\beta) & = (tt_a'|_{\{t | tt_a''(t) > 0\}})^{-1}(\beta) \\
      b_e(\beta) & = t | tt_a(t) = \beta (t - b_i(\beta)) + tt_a(b_i(\beta)),\ t > b_i(\beta) \\
      g_e(\gamma) & = (tt_a'|_{\{t | tt_a''(t) > 0\}})^{-1}(-\gamma) \\
      g_i(\gamma) & = t | tt_a(t) = \gamma (g_e(\gamma) - t) + tt_a(g_e(\gamma)),\ t < g_e(\gamma)
    \end{split}
  \end{equation}
\item If \(b_e(\beta) < g_i(\gamma)\), observation~\ref{obs:char_opt_time_simple} can be used:
  given \(t^*\), the global minimum is realized by
    \begin{equation*}
    t^{opt} =
    \begin{cases}
      b_i(\beta) & \text{if } t^*\in(b_i(\beta), b_e(\beta)) \\
      g_e(\gamma) & \text{if } t^*\in(g_i(\gamma), g_e(\gamma)) \\
      t^* & \text{otherwise}
    \end{cases}
  \end{equation*}
\item Otherwise, the point \(t_s\) is computed as of~\eqref{eq:t_s_numbers}:
  \begin{equation*}
    t_s(\beta, \gamma) = \frac{\beta b_i(\beta) - tt_a(b_i(\beta)) + \gamma g_e(\gamma) + tt_a(g_e(\gamma))}{(\beta + \gamma)}
  \end{equation*}
\item Observation~\ref{obs:char_opt_time_comp} can finally be used:
  the global minimum will be realized by
  \begin{equation*}
    t^{opt} =
    \begin{cases}
      b_i(\beta) & \text{if } t^*\in(b_i(\beta), t_s(\beta, \gamma)) \\
      g_e(\gamma) & \text{if } t^*\in(t_s(\beta, \gamma), g_e(\gamma)) \\
      t^* & \text{otherwise}
    \end{cases}
  \end{equation*}
\end{enumerate}

\begin{figure}
  \centering
  \includegraphics[width=.8\textwidth]{monotone_t_a}
  \caption{The dependency of the optimal arrival time \(t^{opt}\) on the desired arrival time \(t^*\).
    As can be seen, on-time arrivals are realized everywhere but in the intervals described by the points defined below.
    The shaded zones display where early, late and on-time arrivals occur.
    Note that the time of early and late arrivals, while varying on the parameters \(\beta, \gamma\),
  is constant on the desired arrival time.}
  \label{fig:t_opt_on_star}
\end{figure}

The dependency on the solution on the parameter \(t^*\) is thus completely explained:
when increasing \(t^*\), the departure time \(t^{opt}\) will be piecewise the identity,
and constant when not.
Figure~\ref{fig:t_opt_on_star} shows this dependency, in the case in which \(g_i(\gamma) < b_e(\beta)\):
no on-time arrivals are indeed there realized, as can be seen by the absence of a growing part in the middle of the plot.

On top of characterizing the dependency of \(t^{opt}\) on \(t^*\),
the dependency on the parameters \(\beta, \gamma\) is characterized as well, through the functions defined in~\eqref{eq:def_b_ieg_ie}.

Before concluding the section, the following proposition gives a deeper understanding of how this dependency is realized
(on top of being useful in the next section)
\begin{prop}
  \label{prop:monotonous_interv}
  Let \(tt_a\) be a proper travel time function.

  The interval \((b_i(\beta), b_e(\beta))\) is decreasing in \(\beta\) (with respect to the ordering given by being contained).

  Similarly, the interval \((g_i(\gamma), g_e(\gamma))\) is decreasing in \(\gamma\).
\end{prop}
\begin{proof}
  The proof is here given for the early case (with \(\beta\)).
  To prove the late case, the reasoning is exactly symmetrical.

  Consider thus \(\beta_1 > \beta_2\).
  The claim of the proposition is that
  \[(b_i(\beta_1), b_e(\beta_1)) \in (b_i(\beta_2), b_e(\beta_2))\]
  that is equivalent to
  \begin{equation*}
    b_i(\beta_1) > b_i(\beta_2),\qquad b_e(\beta_1) < b_e(\beta_2)
  \end{equation*}

  Consider initially the claim \(b_i(\beta_1) > b_i(\beta_2)\).

  Recall that,
  in the points of the travel time function in which the image of the function \(b_i\) lies,
  the travel time function is convex, and its derivative is thus increasing.
  Since the derivative is increasing, inverting it (that is substantially what the function \(b_i\) does) yields another increasing function:
  this means that, since \(\beta_1 > \beta_2\),
  \begin{equation*}
    b_i(\beta_1) > b_i(\beta_2)
  \end{equation*}

  Consider now the second claim, that \(b_e(\beta_1) < b_e(\beta_2)\).

  This follows, as well, directly from the convexity of the function in the image of \(b_i\):
  being the function convex,
  the tangents to the function \(tt_a\) at the points \(b_i(\beta_1), b_i(\beta_2)\) will intersect in a point between them.
  Having the tangent at point \(b_i(\beta_1)\) a higher slope,
  it will be higher than the other line at any point after the intersection.

  Observing that \(b_e(\beta_1) \geq b_i(\beta_1)\) concludes the proof.
\end{proof}

This proposition is interesting:
it indeed shows that, as \(\beta\) is increased,
the window of \(t^*\) for which an early arrival is realized shrinks (and, similarly, the same happens for \(\gamma\)).

This concludes the section about the analytical minimization of the cost function.
The next section will study how, given distributed parameters \(\beta, \gamma, t^*\),
the likelihood of a data point can be analytically computed.

\section{Computation of the Likelihood}
\label{sec:lik}

Once the theory has been developed,
the likelihood can be analytically estimated.

For introducing the statistical framework,
it is important to understand what are the parameters that introduce stochasticity in the model.

\subsection{Stocasticity in the Parameters}

As introduced in section~\ref{sec:thesis_obj}, the parameters \(\beta, \gamma, t^*\) will be considered as random variables
with probability density functions, respectively, \(f_\beta, f_\gamma, f_{T^*}\).

The first important question to ask is wether the parameters can be considered as independent.

Clearly, the parameters \(\beta, \gamma\) are not independent:
rich people will, in general, have them higher while poorer people will have them lower.
This suggests that the positive correlation between them is not negligible at all.

Recall thus that, in the theory developed in this pages,
the value of the parameter \(\alpha\) has been normalized to one:
supposing \(\beta/\alpha, \gamma/\alpha\) to be independent is way more reasonable than supposing \(\beta, \gamma\) to be so.
This assumption is thus made. For a discussion on wether groups of people have dependant \(\beta/\alpha, \gamma/\alpha\), see \cite{LindseyBook}.

Moreover, the distribution of the parameter \(t^*\) is assumed to be independent from the schedule delay preferences.
Again, some insights on the studied population could be used to confirm or deny this hypothesis.
Being the method anyway embryonic and general, this hypothesis is made as well.

Once the parameters are assumed to be distributed,
the likelihood of an arrival time can be computed.
The next section introduces the computations that will be made,
and gives an high level description of the problem.

\subsection{Description of the Likelihood}
\label{sec:descr-likel}

As explained in section~\ref{sec:lik_comp},
the goal of this section is describing how the Probability Density Function for the random variable \(T^{opt}\), \(f_{T^{opt}}\), can be computed.

In other words, given an arrival time \(t_a\),
the likelihood function has to express how likely that time is to be an actual arrival,
given that the parameters \(\beta, \gamma, t^*\) are distributed according to their density functions \(f_\beta, f_\gamma, f_{T^*}\).

First of all, note that an arrival time can only be an early, late or on-time arrival.
Define thus the random variable \(Q\),
which can take values in \(\{-1, 0, 1\}\) and for which
\(Q = 0\) if the time \(t_a\) is an on-time arrival,
\(Q = -1\) if the arrival time \(t_a\) is a realization of an early arrival and, lastly,
\(Q = 1\) if \(t_a\) is a late arrival.

The probability of a realization \(T^{opt} = t_a\) will thus be
\begin{multline}
  \label{eq:decomposition_prob}
  \prob(T^{opt} = t_a) = \prob(T^{opt} = t_a, Q = -1) + \\
  + \prob(T^{opt} = t_a | Q = 0)\prob(Q = 0)
  + \prob(T^{opt} = t_a, Q = 1)
\end{multline}
that induces, for the Probability Density Functions,

\begin{equation}
  \label{eq:decomposition_pdf}
  f_{T^{opt}}(t_a) = f_{T^{opt}, Q}(t_a, -1)
  + f_{T^{opt} | Q}(t_a | 0)\mathbb{P}(Q = 0) + f_{T^{opt}, Q}(t_a, 1)
\end{equation}

The members of equation~\eqref{eq:decomposition_pdf} can be studied separately:
first of all,
the likelihood of on-time arrivals \(f_{T^{opt} | Q}(t_a | 0)\mathbb{P}(Q = 0)\) will be studied.

After that, an expression for the likelihood for early and late arrivals will be found.

\subsection{Likelihood of On-Time Arrivals}
\label{sec:lik_ot}

As seen in section \ref{sec:glob_min},
on-time arrivals occur when the parameter \(t^*\) is out of some intervals defined by the parameters \(\beta\), \(\gamma\).
Given an arrival time \(t_a\),
it is an on-time arrival if it is equal to \(t^*\) and, moreover, it is out of the said intervals.

More formally, the event \textit{being an on-time arrival} can be decomposed in \textit{being a realization of the random variable \(T^*\)} and \textit{not being into an interval that would yield an internal minimum}.

These events are independent, since \(T^*\) is assumed to be independent from the variables \(\gamma\) and \(\beta\) that decide the intervals.

Since the events are independent, their probabilities can be multiplied.
The Probability Density Function for the conditional random variable will thus be

\begin{equation}
  \label{eq:pdf_ot}
    f_{T_a | Q}(t_a | 0) = \frac{f_{T^*}(t_a)\mathbb{P}( t_a \not\in (b_i(\beta), b_e(\beta)) \cup (g_i(\gamma), g_e(\gamma)))}{\int_{-\infty}^\infty f_{T^*}(t_a)\mathbb{P}( t_a \not\in (b_i(\beta), b_e(\beta)) \cup (g_i(\gamma), g_e(\gamma)))dt_a}
\end{equation}
where the denominator is nothing but a normalization constant, found by integrating the numerator over all the domain of \(t_a\).

Clearly, \(f_{T^*}\) is known.

For the probability of not being in the interval, on the other hand, we have 

\begin{equation}
  \label{eq:prob_not_interv}
  \mathbb{P}( t_a \not\in (b_i(\beta), b_e(\beta)) \cup (g_i(\gamma), g_e(\gamma)))dt_a = \int_{b\in \mathbb{R} \vert t_a \not\in (b_i(b), b_e(b))}\int_{g \in \mathbb{R} \vert t_a \not\in (g_i(g), g_e(g))}f_\beta(b)f_\gamma(g)\, dg\, db
\end{equation}

It is now possible to take advantage of proposition~\ref{prop:monotonous_interv}:
the monotonicity of the intervals imply that there exist two functions
\begin{equation}
  \label{eq:def_b_0_g_0}
  \begin{split}
    \beta_0:\R&\rightarrow(0, \beta_{max}) \\
    t&\mapsto \sup\{b | t \in (b_i(b), b_e(b))\} \\[1em]
    \gamma_0:\R&\rightarrow(0, \gamma_{max}) \\
    t&\mapsto \sup\{g | t \in (g_i(g), g_e(g))\}
  \end{split}
\end{equation}
where
\begin{align*}
  \beta_{max} = \max_t tt_a'(t) \\
  \gamma_{max} = -\min_t tt_a'(t)
\end{align*}

The function defined in this way satisfy
\begin{align}
  \label{eq:threshold_def}
  \begin{split}
     t_a \not\in (b_i(\beta), b_e(\beta)) \iff \beta > \beta_0(t_a) \\
    t_a \not\in (g_i(\gamma), g_e(\gamma)) \iff \gamma > \gamma_0(t_a)
  \end{split}
\end{align}

Note that the codomain of the two functions is well defined:
the values \(\beta_{max}, \gamma_{max}\) limit indeed the values that can be taken by the parameters \(\beta\), \(\gamma\) before a situation in which no early
(or late, depending on the parameter)
arrival is possible anymore.

By using these functions, the expression in \eqref{eq:prob_not_interv} can be simplified, becoming
\begin{equation}
  \label{eq:prob_not_interv_simpl}
  \mathbb{P}( t_a \not\in (b_i(\beta), b_e(\beta)) \cup (g_i(\gamma), g_e(\gamma)))dt_a = \int_{\beta_0(t_a)}^{\beta_{\text{max}}}\int_{\gamma_0(t_a)}^{\gamma_{\text{max}}}f_\beta(b)f_\gamma(g)\, dg\, db
\end{equation}

Plugging this into \eqref{eq:pdf_ot} yields
\begin{equation}
  \label{eq:pdf_ot_simpl}
  f_{T_a | Q}(t_a | 0) = \frac{f_{T^*}(t_a)\int_{\beta_0(t_a)}^{\beta_\text{max}}f_\beta(b)\, db\int_{\gamma_0(t_a)}^{\gamma_\text{max}}f_\gamma(g)\, dg}{\int_{-\infty}^\infty f_{T^*}(t_a)\int_{\beta_0(t_a)}^{\beta_\text{max}}f_\beta(b)\, db\int_{\gamma_0(t_a)}^{\gamma_\text{max}}f_\gamma(g)\, dgdt_a}
\end{equation}

Note now that the denominator of equation~\eqref{eq:pdf_ot_simpl} is exactly equal to \(\prob(Q = 0)\).

The final expression for the likelihood of a point being an on-time arrival is thus
\begin{equation}
  \label{eq:lik_ot}
  f_{T^{opt} | Q}(t_a | 0)\mathbb{P}(Q = 0) = f_{T^*}(t_a)\int_{\beta_0(t_a)}^{\beta_\text{max}}f_\beta(b)\, db\int_{\gamma_0(t_a)}^{\gamma_\text{max}}f_\gamma(g)\, dg
\end{equation}

\subsection{Likelihood for Early and Late arrivals}
\label{sec:lik_early_late}

Consider now the probability \(f_{T^{opt}, Q}(t_a, -1)\) of a point being an arrival and being an early arrival.

A reasoning analogous to what done in section~\ref{sec:lik_ot} shows that an early arrival is realized when,
differently from an on-time arrival,
the point \(t^*\) falls into the interval \((b_i(\beta), \min\{b_e(\beta), t_s(\beta, \gamma)\})\) and,
on top of this, the point \(b_i(\beta)\) coincides with \(t_a\).

These events are not anymore independent,
and the conditional probabilities have to be computed.

The member of the likelihood relative to the early arrival is thus
\begin{align*}
    f_{T_a, \hat{Q}}(t_a, -1) & = \mathbb{P}(t^* \in (b_i(\beta), \min(b_e(\beta), t_s(\beta, \gamma)))) | b_i(\beta) = t_a)f_\beta(tt_a'(t_a))[tt_a''(t_a)]^+ \\
  & = \mathbb{P}(t^* \in (t_a, \min(b_e(tt_a'(t_a)), t_s(tt_a'(t_a), \gamma))))f_\beta(tt_a'(t_a))[tt_a''(t_a)]^+ \\
  & = \int_0^\infty f_\gamma(x) (F_{T^*}(\min(b_e(tt_a'(t_a)), t_s(tt_a'(t_a), x))) - F_{T^*}(t_a)) dx\ f_\beta(tt_a'(t_a))[tt_a''(t_a)]^+\tag{\theequation}\label{eq:lik_early}
\end{align*}

Similarly, if considering late arrivals,

\begin{equation}
  \label{eq:lik_late}
  f_{T_a, \hat{Q}}(t_a, 1) = \int_0^\infty f_\beta(x) (F_{T^*}(t_a) - F_{T^*}(\max(g_i(-tt_a'(t_a)), t_s(x, -tt_a'(t_a)))) ) dx\ f_\beta(tt_a'(t_a))[tt_a''(t_a)]^+
\end{equation}

The various members of equation~\eqref{eq:decomposition_pdf} have thus been computed.
Summing them together yields an expression for the total likelihood,
as wanted at the beginning of the section.

\subsection{Final Expression for the Likelihood}
\label{sec:final_lik}

By plugging \eqref{eq:lik_ot}, \eqref{eq:lik_late} and \eqref{eq:lik_early} into \eqref{eq:decomposition_pdf},
I get a final expression for the likelihood function:
\begin{multline}
  \label{eq:lik_final}
  f_{T_a}(t_a) = f_{T^*}(t_a)\int_{\beta_0(t_a)}^{\beta_\text{max}}f_\beta(b)\, db\int_{\gamma_0(t_a)}^{\gamma_\text{max}}f_\gamma(g)\, dg \\
  + f_\beta(tt_a'(t_a))[tt_a''(t_a)]^+\int_0^\infty f_\gamma(x) (F_{T^*}(\min(b_e(tt_a'(t_a)), t_s(tt_a'(t_a), x))) - F_{T^*}(t_a)) dx\  \\
  + f_\beta(tt_a'(t_a))[tt_a''(t_a)]^+  \int_0^\infty f_\beta(x) (F_{T^*}(t_a) - F_{T^*}(\max(g_i(-tt_a'(t_a)), t_s(x, -tt_a'(t_a)))) ) dx\ 
\end{multline}

The expression is quite convoluted, but is a close form analytical expression
(given the definitions of the auxiliary functions given in the previous sections),
and can be numerically approximated.
The next section shows how the approximation can be made.

\subsection{Numerical Computation of the Likelihood}
\label{sec:numerical_lik}

For the reasons noted in section~\ref{sec:lik_speed}
the result of the expression for the likelihood in~\eqref{eq:lik_final} can't be directly computed.
There are indeed several problems, listed below:
\begin{enumerate}
\item The integrals cannot be analytically computed
\item The functions \(b_i, g_e\) require inverting the derivative of the travel time function \(tt_a'\),
  where it is convex.
\item The functions \(b_e, g_i\), depending on the expression of the travel time function \(tt_a\),
  may be impossible to compute directly, as can be seen from their definition in~\eqref{eq:def_b_ieg_ie}.
\item The functions \(\beta_0, \gamma_0\), as defined in~\eqref{eq:def_b_0_g_0},
  are the result of a maximization over an infinite set.
\end{enumerate}

These problems prevent a numerical computation of the solution via the direct implementation of a closed form analytical expression.
Each one of them can anyway be solved in a different way:
\begin{enumerate}
\item The integrals can be approximated via the trapezoid rule,
  to a desired precision.
\item The derivative of the travel time function is strictly monotone where it has to be inverted.
  A bisection algorithm can thus find the solution.

  Otherwise, a gradient descent algorithm can be directly implemented on a shifted travel time function:
  looking for the point \(t^\beta\)satisfying
  \begin{equation*}
    tt_a'(t^\beta) = \beta
  \end{equation*}
  and supposing that the function is, in \(t^\beta\), convex,
  is equivalent to looking for the minimum of the function
  \begin{equation*}
    tt_a(t) - \beta t
  \end{equation*}

  This minimization problem can be solved via a gradient descent algorithm.

  Again, both methods can solve the problem up to a desired precision.
\item The functions \(b_e, g_i\) can be again approximated via a bisection algorithm:
  they are indeed determined by the point in which a line intersects a function,
  and this intersection is realized only once.
  Finding suitable initial conditions is thus easily doable.
\item Finally, the functions \(\beta_0, \gamma_0\) can again be approximate via a bisection algorithm.
  It is indeed clear that \(0 < \beta_0(t) < \beta_{max}\):
  by using these initial condition,
  at each iteration wether the point \(t\) falls into the interval defined in~\eqref{eq:def_b_0_g_0} can be checked.
  The bisection will be made accordingly.
\end{enumerate}

These operations, while being able to precisely approximate the wanted points,
are all iterative algorithms and are thus extremely resource-heavy,
in particular if performed several times
(note that one of these operations may be performed on the result of another one of them,
multiplying the computational complexity of the two operations).

Since python was decided to be the language of choice for this problem
(due to its flexibility and suitability to work with data from different sources),
the use of an high-performance library was needed.

The choice of JAX allowed to reduce the execution time of the likelihood computation by up to four orders of magnitude when compared to standard python code,
by being able to compile the code and to partially parallelize it.

The numerical approximation of the likelihood is thus entirely written in JAX,
and its implementation can be found \todo{Add link}here

Once the likelihood has been computed,
minimizing it yields the model described in the introduction.
It is thus now possible to use the developed evaluation techniques,
to study how the estimator performs on different scenarios.

\section{Evaluation on Synthetic Dataset}
\label{sec:eval_synth_dataset}

As mentioned in section~\ref{sec:synth_dataset},
the evaluation on synthetic dataset can give some insights on sow the model works,
but doesn't help in proving its efficiency with real data,
that may be, a priori, extremely different from what simulated.

The insights this method gives on how the model works in the simplest case may anyway be helpful:
it will indeed be possible to plot an empirical likelihood function to compare it with the theoretical result,
and the optimizers will be calibrated to achieve a good convergence.

In order to build a synthetic dataset as explained in section~\ref{sec:synth_dataset},
it is necessary to sample from the distributions of the parameters \(\beta, \gamma, t^*\).
The distribution of the parameters,
and the parameter \(\theta\) to be estimated (which parametrizes the distributions),
have thus to be defined.

\subsection{Parameters to Estimate}

Not many examples exist in the literature regarding what kind of distribution the parameters follow.
Recently, \textcite{https://doi.org/10.1111/iere.12692} suppose a bizarre distribution,
consisting of a small number of commuters with some really high values of \(\beta, \gamma\),
differently from the majority of them.
This distribution would have really different mean and variance,
allowing the majority of the users to behave differently from the mean user.
Behaving that the mean user should, on the other hand,
be close enough to the majority of them, we suppose
(for the modelled group) the parameters to be normally distributed
(truncated not to allow impossible values, such as \(\beta, \gamma < 0\)).
Moreover, the distributions of the parameters \(\beta, \gamma\) are assumed to have the same variance.

Being the distribution completely characterized by its mean and variance,
these will be the parameters to be estimated:

\begin{equation}
  \label{eq:theta_def}
  \theta = (\mu_\beta, \mu_\gamma, \mu_t, \sigma, \sigma_t)
\end{equation}

Where \(\mu_\beta, \mu_\gamma, \mu_t\) are the means of the parameters \(\beta, \gamma, t^*\),
\(\sigma_t\) is the variance of \(t^*\) and \(\sigma\) the variance of \(\beta, \gamma\).

Fixing a value for the parameter \(\theta\),
it becomes possible to sample from these distributions,
and to consequently build the synthetic dataset.

The next section shows how this can be done, and the distribution yielded from this procedure.

\subsection{The Synthetic Dataset}


\begin{figure}
  \centering
  \includegraphics[width=.7\textwidth]{t_as_bins_tt}
  \caption{Histogram depicting the distribution of the arrival times when directly minimizing the cost function.
    The green and red bands show where (respectively) early and late arrivals are expected to occur,
    and the higher density of points in these zones confirm this.
  For obtaining this distribution, a value of \(\theta = (0.8, 1.4, 9.5, 0.3, 1)\) has been used.}
  \label{fig:t_as}
\end{figure}

Figure~\ref{fig:t_as} shows the result of building the dataset, given a travel time function and a value for the parameter \(\theta\).
As expected, higher density zones are observed where early and late arrivals are expected to be realized
(that is, when the derivative of the travel time function is close to the mean of the parameters \(\beta, -\gamma\)).

\begin{figure}
  \centering
  \includegraphics[width=.8\textwidth]{hist_ll}
  \caption{Histogram of the arrival times when sampling a big number of points (in this case, 100,000 points are sampled),
    plotted with a scaled version of the likelihood function.
    It can be seen how the theoretically computed likelihood closely follows the empirical distribution of the points,
    confirming the accuracy of the theoretical work done in section~\ref{sec:lik}.}
  \label{fig:hist_pdf}
\end{figure}

To show whether this characteristic is properly described by the theoretical likelihood developed in section~\ref{sec:lik},
note that the histogram of the empirical distribution should converge,
as the sample size increases, to the shape of the probability density function.
The probability density function \eqref{eq:lik_final} is thus plotted,
alongside an histogram made by sampling a big number of arrival times,
in figure~\ref{fig:hist_pdf}.







%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
